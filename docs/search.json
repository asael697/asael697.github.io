[
  {
    "objectID": "ABM/posts/ABR/index.html",
    "href": "ABM/posts/ABR/index.html",
    "title": "Applied Bayesian Regression",
    "section": "",
    "text": "Este post presenta el contenido, direcciones y detalles de “Applied Bayesian Regression.”\nLa segunda parte del curso se impartirá regresión Bayesiana aplicada, el objetivo principal es que el estudiante pueda resolver un problema de regresión con un conjunto de datos reales mediante un ordenador, independiente del enfoque de inferencia a utilizar.\nEl material de la clase se encuentra en el repositorio Applied-Bayesian-Regression. Este presenta el contenido y cuadernos para la clase de Modelos lineales para la Maestría en Matemática, tercer periodo del año 2021.\nLa clase se imparte los Lunes, Martes y Jueves a las 17:00 horas (GMT -6), Para acceder al enlace zoom de la clase presione aquí."
  },
  {
    "objectID": "ABM/posts/ABR/index.html#contenido",
    "href": "ABM/posts/ABR/index.html#contenido",
    "title": "Applied Bayesian Regression",
    "section": "Contenido",
    "text": "Contenido\nEl contenido para el resto del curso es:\n\nRegresión aplicada\n\nVerosimilitud y función de enlace.\nL-BFGS algorithm.\nIntervalos de confianza, Jackniffe y Bootstrap.\nAnálisis de residuos, ANOVA y R^2 ajustado.\nselección del modelos (BIC, RMSE, MAPE, CV)\n\nRepaso de inferencia Bayesiana y Bayesian workflow\n\nPrior, likelihood, Posterior\nMCMC\nPredictive distribution\nPosterior predictive checks\nBayes factor, ELPD, LOO-CV.\n\nBayesian Regression\n\nBayesian GLMs, normal, Binomial, Poisson and Negative Binomial regressions\nRegularized priors (R2-D2, Horseshoe, Spike Lab)\nGaussian process regression."
  },
  {
    "objectID": "ABM/posts/ABR/index.html#material",
    "href": "ABM/posts/ABR/index.html#material",
    "title": "Applied Bayesian Regression",
    "section": "Material",
    "text": "Material\nEl material de la clase se extrae de 3 libros, varios artículos y diferentes paquetes de R y Python estos son libres y se encuentran en formato digital en la web.\n\nLibros\n\nBayes Rules! An Introduction to Applied Bayesian Modeling. Johnson, Ott and Dogucu, (2021).\nBeyond Multiple Linear Regression Applied Generalized Linear Models and Multilevel Models in R. Roback and Legler (2021).\nBayesian Modeling and Computation in Python. Martin, Kumar, and Lao (2021).\n\n\n\nArtículos principales\n\nBayesian Regression Using a Prior on the Model Fit: The R2-D2 shrinkage Prior. Zhang et al. (2022)\nHandling Sparsity via the Horseshoe. Carvalho, Polson, and Scott (2009)\nBayesian Variable Selection in Linear Regression. Mitchell and Beauchamp (1988)\n\n\n\nPaquetes\nLos lenguajes de programación a usar son R y Python.\n\n\nR core team\n\nProbabilistic Programming Language: Stan mc-stan.\npaquetes:\n\nrstanarm, paquete para ajustar modelos lineales.\nbayesplot, visualización de posterioris.\nloo seleccion de modelos.\n\n\n\n\nPython\n\nProbabilistic Programming Language: PyMC.\npaquetes:\n\nBambi, Ajustar modelos lineales.\nArviZ, visualización de datos y selección de modelos con LOO."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html",
    "href": "ABM/posts/GLMs/index.html",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "",
    "text": "Este post resume el workflow básico para realizar un análisis estadístico en un enfoque clásico.\nEl modelo lineal generalizado (GLM) relaciona de forma funcional un conjunto de variables aleatorias Z = (Y,X), donde la v.a. Y \\in \\mathbb{R} se le conoce como variable dependiente, y a la v.a. X \\in \\mathbb{R}^d son las covariables.\nSea Z = \\{Z_1,Z_2,\\ldots,Z_n\\} un conjunto de variables aleatorias independientes, tal que Z_i = (Y_i,X_i) \\in \\mathbb R^{d+1}, decimos que Z sigue un GLM si:\nY_i \\sim \\mathscr{F}_\\varepsilon (\\theta_i), \\quad \\text{y } g(\\theta_i) = \\beta X_i. Donde:"
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#log-verosimilitud",
    "href": "ABM/posts/GLMs/index.html#log-verosimilitud",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Log-Verosimilitud",
    "text": "Log-Verosimilitud\nLos GLMs son modelos probabilistas, cuya función de probabilidad se establece mediante la verosimilitud en escala logarítmica. Dado el supuesto de independencia de los datos, la verosimilitud es simplemente el producto de las densidades marginales de los datos.\nL(y;\\theta) = f(y_1,y_2,\\ldots,y_n | \\theta) = \\prod_{i=1}^nf(y_i\\theta).\nDado que los GLM pertenecen a la familia exponencial, la verosimilitud se puede expresar de forma analítica como:\nL(y;\\theta) = \\prod_{i=1}^n \\exp \\left[y_i b(\\theta_i)+c(\\theta_i)+d(y_i) \\right].\nFinalmente, la log-verosimilitud que es el logaritmo de L, también posee forma analítica\nl(y;\\theta) = \\sum_{i=1}^n y_i b(\\theta_i) +\\sum_{i=1}^nc(\\theta_i) + \\sum_{i=1}^n d(y_i)."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#estimación-de-los-parámetros",
    "href": "ABM/posts/GLMs/index.html#estimación-de-los-parámetros",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Estimación de los parámetros",
    "text": "Estimación de los parámetros\nEl método de optimización más popular en GLMs es Máxima Verosimilitud que consiste en optimizar la log-verosimilitud, o simplemente resolver la función de score.\nU(y;\\theta) = \\frac{\\partial}{\\partial \\theta} l(y;\\theta) = 0. Generalmente resolver U implica resolver un sistema de ecuaciones no lineales, y al inicio el método más utilizado es el algoritmo de Newton. En la actualidad dicho algoritmo a evolucionado a un Limited Memory Broyden–Fletcher–Goldfarb–Shanno algorithm. El algoritmo L-BFGS es un método de Quasi-Newton que\n\nAproxima la matriz Jacobiana usando el método de Broyden, y\nAplica el algoritmo de Fletcher para corregir por estabilidad la aproximación de Broyden."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#incertidumbre-de-los-estimadores",
    "href": "ABM/posts/GLMs/index.html#incertidumbre-de-los-estimadores",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Incertidumbre de los estimadores",
    "text": "Incertidumbre de los estimadores\nUn estimador es cualquier estadístico w(Y) que se utiliza para inferir información de \\beta. Dado que los estimadores \\hat \\beta = w(Y) son transformaciones de la muestra, poseen una distribución1.\nw(Y) \\sim f(w|\\beta).\nEn la mayoría de los casos, la distribución muestral no tiene forma analítica, y esta se aproxima usando remuestreo.\nLos algoritmos de remuestreo más utilizados son\n\nJackniffe\nBootstrap\n\nDe los dos algoritmos el más popular es Bootstrap, que consiste en generar una muestra de estimadores que aproxima la distribución deseada. El algoritmo es:\n\nElegir el número de sub-muestras B\nPara b =1,2,3,\\ldots, B hacer:\n\nExtraer una sub-muestra Y_b con reemplazo de Y,\nEstimar los parámetros del modelo \\hat \\beta_b con la sub-muestra Y_b.\n\nLa colección \\hat \\beta_1,\\hat \\beta_2, \\hat \\beta_3 ,\\ldots,\\hat \\beta_B es una muestra de la distribución muestral de los estimadores \\hat \\beta.\nUsar \\hat \\beta_1,\\hat \\beta_2, \\hat \\beta_3 ,\\ldots,\\hat \\beta_B para aproximar los intervalos de confianza de \\hat \\beta.\n\nEl método de Jackniffe es un caso particular del Bootstrap que consiste en extraer la sub-muestra Y_b eliminando la b-ésima observación y_b de la muestra original.\nPara análisis de incertidumbre de los estimadores, el Bootstrap provee mejores resultados que el método de Jackniffe, pero el segundo tiene su nicho al comparar diferentes modelos, validación cruzada."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#análisis-de-los-residuos.",
    "href": "ABM/posts/GLMs/index.html#análisis-de-los-residuos.",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Análisis de los residuos.",
    "text": "Análisis de los residuos.\nLos residuos de un modelo se definen como la diferencia entre el valor ajustado por el modelo \\hat Y y su valor real Y.\nR_i = \\hat Y_i - Y_i.\nEn GLMs solo se revisan generalidades simples de los supuestos como:\n\nQue estén centrados en cero R_i \\approx 0.\nSean de varianza homogénea \\sigma_R\n\nEn modelos lineales simples, se pueden revisar más supuestos como normalidad, homogeneidad, y estacionaridad, ya que los residuos estiman los errores del modelo R_i = \\hat \\varepsilon_i.\nUn estadístico importante obtenido de los residuos, es el coeficiente de determinación R^2 que establece el porcentaje de varianza explicada por el modelo.\nR^2 = 1 - \\frac{\\sigma_R/(n-d-1)}{V[y]/(n-1)}"
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#selección-de-modelos",
    "href": "ABM/posts/GLMs/index.html#selección-de-modelos",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Selección de modelos",
    "text": "Selección de modelos\nEn la práctica, es posible que se desarrollen múltiples modelos que expliquen el mismo conjunto de datos Z, existen indicadores que permiten describir las cualidades del modelo, los más utilizados son:\n\nlog-verosimilitud: \\log L= -l(y;\\theta).\nCriterio de Información de Bayes: BIC = 2 \\log L - 2\\log(n^d)\nCriterio de Información de Akaike AIC - 2 \\log L -2 d\nRoot Mean square error RMSE = \\frac{1}{\\sqrt n}||R||_2\nMean Absolute Error MAE = \\frac{1}{n}||R||_1\nMean Absolute Percentaje Error MAPE =\\frac{1}{n}\\sum_{i=1}^n \\left| \\frac{R_i}{Y_i} \\right|\n\n\nSelección de variables\nUna aplicación de la selección de modelos es encontrar modelos reducidos, esto es encontrar un subconjunto de variables X_1 \\subset X tal que el modelo reducido M_r sea lo mas parsimonioso posible. Un modelo parsimonioso brinda mayor explicabilidad, mayor capacidad de aprendizaje y mayor capacidad de generalización.\nLos métodos para reducción de variables se les conoce como búsquedas, y los tipos son:\n\nBúsquedas hacia adelante\nBúsquedas hacia atrás.\n\nEn búsqueda hacia adelante se inicia con el modelo más pequeño posible, se mide su criterio de información, y luego se agregan variables de forma secuencial de tal forma que el criterio mejore."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#aprendizaje",
    "href": "ABM/posts/GLMs/index.html#aprendizaje",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Aprendizaje",
    "text": "Aprendizaje\nLos indicadores presentados miden el ajuste del modelo, esto es la capacidad del modelo de explicar el conjunto de datos. En aplicaciones mas recientes, la explicabilidad es una propiedad poco deseable, se prefiere medir la capacidad de aprendizaje.\n\n\n\n\n\n\nAprendizaje\n\n\n\nLa capacidad de aprendizaje es la habilidad del modelo de explicar propiedades externas a partir de la información adquirida.\n\n\nEn términos probabilistas, es la habilidad del modelo de predecir un nuevo conjunto de datos, a partir de los datos disponibles.\n\nParticiones\nEl aprendizaje se puede medir usando una partición de los datos. Sea Y = \\{Y_1,Y_2,\\ldots,Y_n\\} una muestra aleatoria, definimos una partición de entrenamiento y prueba como\nY = Y_E \\bigcup Y_P Donde:\n\nY_E \\bigcap Y_P = \\emptyset.\nm+k = n y m >> k.\nY_E = \\{Y_1,Y_2,\\ldots,Y_m\\} \\subset Y es el conjunto de entrenamiento.\nY_P = \\{Y_1,Y_2,\\ldots,Y_k\\} \\subset Y es el conjunto de prueba.\n\nEl algoritmo para medir aprendizaje es:\n\nAjustar los modelos M_1,M_2,\\ldots,M_f usando el conjunto de entrenamiento Y_E.\nPara cada modelo M_j hacer:\n\nRealizar k predicciones \\hat Y_{P,1},\\hat Y_{P,2},\\ldots,\\hat Y_{P,k}.\nComparar \\hat Y_{P} con Y_P usando AIC, BIC log-lik, RMSE o MAPE.\n\nEl modelo con mayor aprendizaje es el modelo con criterio menor.\n\nLa mayor limitante de las particiones es que al ser aleatorias los resultados varian bastante según la selección de las observaciones en el conjunto de entrenamiento. Una forma de evitar esos problemas es usando Validación cruzada."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#validación-cruzada",
    "href": "ABM/posts/GLMs/index.html#validación-cruzada",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Validación cruzada",
    "text": "Validación cruzada\nValidación Cruzada consiste en realizar el proceso de partición muchas veces, el método mas utilizados es k-fold cross-validation este consisten en:\n\nDefinir m como el número de iteraciones a realizar\nPara i = 1,2,3,\\ldots, m hacer:\n\ndefinir el conjunto de prueba Y_p de tamaño k.\ndefinir el conjunto de entrenamiento Y_E como el complemento Y_E = Y_P^c.\nEstimar los modelos con Y_E\nComparar las predicciones de cada modelo con Y_P mediante algún criterio de información.\n\nPromediar los criterios obtenidos de cada iteración.\nElegir el modelo con criterio promedio menor.\n\nCuando el conjunto e prueba solo posee una observación, al método se le conoce como LOO (Leave one out cross validation). Otro método es utilizar un Bootstrap pero es altamente costoso.\n\nNotar que:\n\nLOO es equivalente a un muestreo de Jackniffe.\nCon validación cruzada re-utilizamos la información\nUsamos toda la muestra para validar los resultados.\nSe minimiza la variación de errores de aprendizaje."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html",
    "href": "ABM/posts/Gaussian-LM/index.html",
    "title": "GLMs Gaussianos: Capacidad de Motores de carros",
    "section": "",
    "text": "Este post presenta un análisis la base mtcars usando un modelo lineal múltiple con verosimilitud Gaussiana.\nLa base de datos mtcars contiene el registro de motores de carros mas populares en USA, 1974. los datos contienen 32 registros, con 10 atributos del motor.\nSe desea predecir la capacidad de consumo de los motores, para eso se evaluaron las siguiente variables."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#verosimilitud",
    "href": "ABM/posts/Gaussian-LM/index.html#verosimilitud",
    "title": "GLMs Gaussianos: Capacidad de Motores de carros",
    "section": "Verosimilitud",
    "text": "Verosimilitud\nPara medir la relación de consumo de los motores utilizaremos un GLM normal tal que:\nmpg_i \\sim N(\\mu_i,\\sigma^2), \\quad  g(\\mu_i) = \\mu_i, \\text{ y } \\mu_i = \\beta X_i.\nEl siguiente código limpia la base de datos para obtener las variables de interés\n\n\nCode\ndf = mtcars[,c(1,4,6,8,10,11)]\nstr(df)\n\n\n'data.frame':   32 obs. of  6 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nTodas las variables son numéricas, pero algunas son totalmente enteras, dificultando el proceso de análisis, se procede a revisar las correlaciones para revisar las interacciones lineales entre variables.\n\n\nCode\nggpairs(df)\n\n\n\n\n\nFigure 1: Gráfico de pares. La diagonal principal muestra histogramas densidades de cada una de las variables. La parte superior muestra el coeficiente de correlación entre dos variables, fila y columna. La parte inferior muestra un gráfico de dispersión entre dos variables.\n\n\n\n\nFigure 1 muestra colinealidad entre las variables mpg, hp y wt. Por lo tanto, múltiples modelos deben ser considerados. Realizemos un modelo inicial, el considerado el modelo completo que posee todas las variables\n\n\n\n\n\n\nColinealidad\n\n\n\nDos covariables X_1 y X_2 se dicen ser colineales si las variables son linealmente dependientes.\nRecordar que si dos columnas de una matriz son linealmente dependiente, entonces el determinante es cero."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#ajuste-del-modelo",
    "href": "ABM/posts/Gaussian-LM/index.html#ajuste-del-modelo",
    "title": "GLMs Gaussianos: Capacidad de Motores de carros",
    "section": "Ajuste del modelo",
    "text": "Ajuste del modelo\nAjustamos el modelo completo que consiste en usar todas las variables, y revisamos el ajuste e inferencia de los parámetros.\n\n\nCode\nm1 = lm(mpg~.,data = df)\nsummary(m1)\n\n\n\nCall:\nlm(formula = mpg ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2884 -1.4370 -0.3155  1.1697  5.8246 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 27.20311    5.74212   4.737 6.72e-05 ***\nhp          -0.02339    0.01353  -1.728   0.0958 .  \nwt          -2.74663    0.92005  -2.985   0.0061 ** \nvs           0.94692    1.36929   0.692   0.4954    \ngear         1.78520    1.12762   1.583   0.1255    \ncarb        -0.65498    0.57767  -1.134   0.2672    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.569 on 26 degrees of freedom\nMultiple R-squared:  0.8477,    Adjusted R-squared:  0.8184 \nF-statistic: 28.94 on 5 and 26 DF,  p-value: 7.653e-10\n\n\nDebido a la alta colinealidad entre las variables, pocos parámetros estimados son significativos. Procedemos a eliminar algunas variables del modelo. Eliminamos la variable wt al ser colineal con múltiples variables. Por lo tanto, el modelo inicial M_1 es:\n\n\nCode\nm1  =  lm(mpg~vs+hp+gear+carb,data = df)\nsummary(m1)\n\n\n\nCall:\nlm(formula = mpg ~ vs + hp + gear + carb, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8047 -2.3487 -0.0967  1.9188  6.7859 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.03756    3.67694   3.546  0.00145 ** \nvs           0.84671    1.55657   0.544  0.59093    \nhp          -0.03449    0.01480  -2.331  0.02747 *  \ngear         4.20129    0.89285   4.705 6.72e-05 ***\ncarb        -1.33338    0.60391  -2.208  0.03593 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.921 on 27 degrees of freedom\nMultiple R-squared:  0.7955,    Adjusted R-squared:  0.7652 \nF-statistic: 26.25 on 4 and 27 DF,  p-value: 5.825e-09"
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#incertidumbre-de-los-estimadores.",
    "href": "ABM/posts/Gaussian-LM/index.html#incertidumbre-de-los-estimadores.",
    "title": "GLMs Gaussianos: Capacidad de Motores de carros",
    "section": "Incertidumbre de los estimadores.",
    "text": "Incertidumbre de los estimadores.\nPese que la función lm de R realiza un análisis de incertidumbre al presentar una Prueba-t de significacia para cada parámetro \\beta_i, no presenta los intervalos de confianza. Estos serán estimados con Bootstrap. La siguiente función obtiene una muestra Bootstrap de los parámetros desconocidos \\beta.\n\n\nCode\nlm_boots = function(y,x,B = 1000){\n  n = length(y)\n  est = NULL\n  for (i in 1:B) {\n    si = sample(x = 1:n,size = n,replace = TRUE)\n    mli = lm(y[si]~x[si,] )\n    ci = as.array(mli$coefficients)\n    est = rbind(est,ci)\n  }\n  # Estética\n  cn = colnames(x)\n  colnames(est) = c(\"intercepto\",cn)\n  \n  return(est)\n}\n\n\nObtenemos una muestra Bootstrap para los estimadores \\hat \\beta de tamaño B=5,000 repeticiones\n\n\nCode\nbtp = lm_boots(y = df$mpg,x = as.matrix(df[,-1]),B = 5000)\n\nbayesplot_theme_set(theme_grey())\nmcmc_dens(btp)+labs(title=\"Distribución muestral de los estimadores\",\n                    subtitle =\"Bootstrap B = 5,000 iteraciones\")\n\n\n\n\n\nFigure 2: Gráfico de densidades. Cada densidad representa la distribución muestral aproximada para cada uno de los estimadores usando un Bootstrap de B=5,000 iteraciones.\n\n\n\n\nLos intervalos de confianza al 95% son:\n\n\nCode\nx = apply(btp,MARGIN = 2, FUN = quantile, probs = c(0.025,0.5,0.975)) \n\n# Estética\nx = data.frame( t(x) )\nx$pars = c(\"intercepto\",\"hp\",\"wt\",\"vs\",\"gear\",\"carb\")\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"parámetros\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\n\nTable 1:  Intervalos de confianza al 95%, obtenidos a partir de una muestra bootstrap de B = 5,000 iteraciones parámetrosq2.5%Medianq97.5%intercepto13.0433674227.0697484337.837299379hp-0.04732486-0.023965780.002413294wt-4.61440854-2.64810785-0.975654924vs-2.041172040.767777173.167562241gear0.206193041.827686955.141965668carb-2.25435361-0.710446720.292475032\n\n\n\nLos intervalos de confianza revelan mayor información a lo obtenido por la prueba-t, parámetros como hp, y carb que son significativos en la prueba, no lo son mediante los intervalos. Esto indica la posibilidad de un modelo mucho mas reducido."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#análisis-de-los-residuos",
    "href": "ABM/posts/Gaussian-LM/index.html#análisis-de-los-residuos",
    "title": "GLMs Gaussianos: Capacidad de Motores de carros",
    "section": "Análisis de los residuos",
    "text": "Análisis de los residuos\nUna vez evaluadas las estimaciones del modelo, es necesario revisar los residuos del mismo para corroborar supuestos, la siguiente linea de código presenta un resumen descriptivo de los residuos del modelo inicial M_1, en su mayoría parecen estar centrados en cero.\n\n\nCode\nsummary(m1$residuals)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-3.80470 -2.34875 -0.09674  0.00000  1.91880  6.78592 \n\n\nFigure 3 presenta una visualización típica para el diagnostico de los residuos, ninguna figura debe presentar un comportamiento polinómico a excepción del gráfico de quantiles (derecha superior), que debe seguir el comportamiento de una función lineal creciente.\n\n\nCode\nautoplot(m1)\n\n\n\n\n\nFigure 3: Gráfico diagnóstico de los residuos, estos cuatro gráficos evaluan el ajuste y supuestos del modelo, si algún comportamiento polinómico es persistente, entonces los supuestos del modelo no se satisfacen."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#selección-de-modelos",
    "href": "ABM/posts/Gaussian-LM/index.html#selección-de-modelos",
    "title": "GLMs Gaussianos: Capacidad de Motores de carros",
    "section": "Selección de modelos",
    "text": "Selección de modelos\nAdicional al modelo M_1, ajustamos dos modelos mas:\nM_2: \\quad mpg \\sim N(hp+gear+carb,\\sigma^2), M_3: \\quad mpg \\sim N(wt+gear+carb,\\sigma^2).\n\n\nCode\nm2 = lm(mpg~hp+gear+carb,data = df)\nm3 = lm(mpg~wt+gear+carb,data = df)\n\n\nEl siguiente código calcula el RMSE de un modelo linea en el conjunto de entrenamiento.\n\n\nCode\nrmse = function(m){\n  mse = sum(m$residuals^2)/length(m$residuals)\n  return(sqrt(mse))\n}\n\n\n\n\nCode\nx = matrix(0,nrow = 4,ncol = 3)\nx[1,] = c(logLik(m1),logLik(m2),logLik(m3))\nx[2,] = c(AIC(m1),AIC(m2),AIC(m3))\nx[3,] = c(BIC(m1),BIC(m2),BIC(m3))\nx[4,] = c(rmse(m1),rmse(m2),rmse(m3))\n\n# Estética\nx = data.frame(x)\nx$pars =  c(\"logLik\",\"AIC\",\"BIC\",\"RMSE\")\ncolnames(x)  = c(\"Modelo 1\",\"Modelo 2\",\"Modelo 3\",\"Criterio\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\n\nTable 2:  Criterios de información de los modelos. Se selecciona el modelo con menores criterios. CriterioModelo 1Modelo 2Modelo 3logLik-76.986353-77.160742-75.196419AIC165.972706164.321484160.392838BIC174.767122171.650163167.721517RMSE2.6828642.6975252.536917\n\n\n\nTable 2 muestra la tabla de criterios de información para el conjunto de datos mtcars para sorpresa del lector el mejor modelo es el alternativo M_3 que usa la variable colineal wt en vez de hp."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#validación-cruzada",
    "href": "ABM/posts/Gaussian-LM/index.html#validación-cruzada",
    "title": "GLMs Gaussianos: Capacidad de Motores de carros",
    "section": "Validación cruzada",
    "text": "Validación cruzada\nEl siguiente código presenta una función para realizar k-fold-CV para cualquier valor de k. En caso de querer añadir otros modelos o criterios, la función deberá ser modificada.\n\n\nCode\nkfold = function(df,k){\n  # Generar la particion\n  kfld = createFolds(df[,1],k = k)\n  mat = NULL\n  \n  for (i in 1:k) {\n    # separar los datos en conjuntos de prueba y entrenamiento\n    dfE= df[-kfld[[i]],]\n    dfP = df[kfld[[i]],]\n    # Ajustar los modelos\n    m1 = lm(mpg~vs+hp+gear+carb,data = dfE)\n    m2 = lm(mpg~hp+gear+carb,data = dfE)\n    m3 = lm(mpg~wt+gear+carb,data = dfE)\n    \n    p1 = predict(m1,dfP)\n    p2 = predict(m2,dfP)\n    p3 = predict(m3,dfP)\n    # Calcular AIC y RMSE\n    aic = c(\n            AIC(m1),\n            AIC(m2),\n            AIC(m3)\n            )\n    rmse = c(\n             RMSE(pred =  p1,obs = dfP[,1]),\n             RMSE(pred =  p2,obs = dfP[,1]),\n             RMSE(pred =  p3,obs = dfP[,1])\n             )\n    mae = c(\n            MAE(pred =  p1,obs = dfP[,1]),\n            MAE(pred =  p2,obs = dfP[,1]),\n            MAE(pred =  p3,obs = dfP[,1])\n            )\n    mape = c(\n              mean(abs((p1-dfP[,1])/dfP[,1])),\n              mean(abs((p2-dfP[,1])/dfP[,1])),\n              mean(abs((p3-dfP[,1])/dfP[,1]))\n              )\n    # Unir los datos\n    mat = rbind(mat,c(aic,rmse,mae,mape)) \n  }\n  colnames(mat) = c(\"AIC1\",\"AIC2\",\"AIC3\",\"RMSE1\",\"RMSE2\",\"RMSE3\",\"MAE1\",\"MAE2\",\n                    \"MAE3\",\"MAPE1\",\"MAPE2\",\"MAPE3\")\n  row.names(mat) = NULL\n  return(mat)\n}\n\n\nTable 3 presenta los resultados obtenidos al realizar 5-fold-cv, bajo todos los criterios presentados, el modelo M_3 presenta las mejores predicciones. Por lo tanto, M_3 es el modelo con Mayor aprendizaje.\n\n\nCode\nrst = kfold(df = df,k = 5)\nx = t(apply(rst,MARGIN = 2,FUN = \"quantile\",probs = c(0.025,0.5,0.975)))\n\n# Estética\nx = data.frame(x)\nx$pars =  c(\"AIC1\",\"AIC2\",\"AIC3\",\"RMSE1\",\"RMSE2\",\"RMSE3\",\"MAE1\",\"MAE2\",\n                    \"MAE3\",\"MAPE1\",\"MAPE2\",\"MAPE3\")\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"Criterio\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\n\nTable 3:  Criterios de información de los modelos. Mediante Validación cruzada. La tabla presenta los criterios AIC, RMSE, MAE y MAPE bajo un 10-fold cv. Criterioq2.5%Medianq97.5%AIC1121.27452574135.8144340141.1333914AIC2119.69800911134.7774906139.4652709AIC3115.27129156133.2425912136.3239395RMSE11.528535663.10939224.7664596RMSE21.529728813.09614164.5683149RMSE31.449069673.31991844.1000376MAE11.167095972.73877634.1490150MAE21.188603682.74190873.9740156MAE31.196918172.53957763.3251070MAPE10.058178710.14064580.2110424MAPE20.058437400.14188910.2016244MAPE30.063046390.11304590.1766864"
  },
  {
    "objectID": "ABM/index.html",
    "href": "ABM/index.html",
    "title": "Applied Bayesian Modeling",
    "section": "",
    "text": "Gaussian\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRegression\n\n\ncode\n\n\nGLMs\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nAsael Alonzo Matamoros.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\nintroduction\n\n\nBaseline\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Asael Alonzo Matamoros",
    "section": "",
    "text": "Hello, I’m Asael, a statistician who programs in R, loves-hates Python, plays way too much Pokemon, talks a lot, is crazy for traveling, and gets depressed often. Welcome to my blog! :). If you want to know more about my work, please check this section where I brag about myself or take home a paper (CV) for free XD.\nImportant: If you are a developer, R fanatic, or statistician, please star my bayesforecast package."
  },
  {
    "objectID": "index.html#asael-an-attempt-of-websiteblog-again",
    "href": "index.html#asael-an-attempt-of-websiteblog-again",
    "title": "Asael Alonzo Matamoros",
    "section": "Asael, an attempt of website/Blog, again?!",
    "text": "Asael, an attempt of website/Blog, again?!\nWell, yes! One of my major flaws is my English writing, so I am doing this website for three different purposes:\n\nImprove my R, Python, and Julia skills by presenting some time-series analyses using those languages.\nPractice my English writing by saturating everyone, including myself, with this bullsh@#t.\nPresent all those little works and collaborations I do in a compact (bounded and closed) set."
  },
  {
    "objectID": "index.html#so-whats-going-on-with-my-life-now",
    "href": "index.html#so-whats-going-on-with-my-life-now",
    "title": "Asael Alonzo Matamoros",
    "section": "So what’s going on with my life now?",
    "text": "So what’s going on with my life now?\nAfter three long years abroad studying, traveling, researching, surviving this sh@#t-show (COVID), snow, and rejections, I am going home! For now, this webpage is my new attempt to organize my mess.\nIf you want to know more about it or are having a heavy season and want somebody to talk to, send me a friend request or mail me. Who knows, you might have a new friend :).\nSpoilers alert: I like to sing out loud with my headphones on in public spaces, so if you find a dude badly singing at a coffee shop in front of the computer, it’s me! Please say hi! :)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I am a Bayesian Data scientist with a strong background in Bayesian theory, probabilistic programming, time-series analysis, actuarial science, risk modeling, mathematical decision-making, and package developing in R.\nKeywords: Forecasting, Bayesian data analyst, Time-series, Probabilistic ML, Mathematics.\n\n\n\nProgramming: R, Python, Julia, Octave, Matlab, C++, SQL, Git.\nStatistical tools: SPSS, STATA, R, Excel, Stan, PyMC, Tableau, PowerBI.\nLanguagues: Spanish (native), English (professional), Dutch (Beginner)."
  },
  {
    "objectID": "about.html#research-projects",
    "href": "about.html#research-projects",
    "title": "About me",
    "section": "Research projects",
    "text": "Research projects\n\nBayesian order identification of ARMA models with projection predictive inference. McLatchie Y., Alonzo Matamoros A., Vehtari A. (2022). In submission to AISTATS 2023.\nUncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison. Sivula T., Magnusson M., Alonzo Matamoros A., Vehtari A. (2022). Submitted to JMLR.\nThe Multiple Degrees of freedom Gaussian Process. Alonzo Matamoros A. (2021). Presented at the Workshop of Bayesian Stochastic Processes.\nBayesian time-series modeling with bayesforecast. Alonzo Matamoros A., Vehtari A. (2021). Presented at the International Society of Bayesian Analysis 2021.\nAn R package for Normality in Stationary Processes. Alonzo Matamoros A, Nieto-Reyes A. (2020). Submitted to Journal of R."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\n\nPh.D. candidate in Computer Science.\nAalto University, Finland | October 2020.\n\n\nM.Sc. in Mathematics and Computer Science.\nUniversidad de Cantabria, Spain | October 2019 - October 2020.\n\n\nCertification in Probability.\nSociety of Actuaries, U.S.A. | November 2016.\n\n\nB. Sc. in Mathematics and Computer Science.\nUniversidad Nacional Autonoma de Honduras | October 2010 - October 2015."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About me",
    "section": "Experience",
    "text": "Experience\n\nResearch contract\nFinnish Center of Artificial Intelligence | October 2020.\nStudied the uncertainty of Cross-Validation methods when comparing probabilistic machine learning models. Proposed an algorithm for automatic forecasting with probabilistic time-series models.\nSkills: R, Python, ML, Model Comparison, Data Visualization..\n\n\nAssistant Professor, Statistics Department\nUniversidad Nacional Autonoma de Honduras (UNAH) | October 2017 – October 2020.\nImparted Statistical Inference, Stochastic Processes, and Linear Models classes for the department of Statistics.\nSkills: Julia, STATA, Mathematics, Bayesian Statistics.\n\n\nData analyst\nInstituto Hondureño de Transporte Público | January 2017 – April 2018.\nDesigned statistical models for estimating the public transport demands using simulation and stochastic processes. Adapted dynamic econometric models to analyze Tegucigalpa, Honduras’s public transport system.\nSkills: Simulation, Probability, Mathematical Modeling, Bayesian Statistics.\n\n\nActuarial analyst\nInstituto Hondureño de Seguridad Social | January 2016 – January 2017.\nWrote scripts for data cleaning and summaries for the actuarial evaluation of the total of affiliates in Honduras’ primary pension system. Created big-data modules for the actuarial analysis of the pension system.\nSkills:SQL, Data-cleaning, Big-data, Actuarial Modeling.\n\n\nStatistical Consultant\nInstituto Hondureño de Turismo | January 2015 – January 2016.\nDesigned the Sampling procedure for estimating the Touristic Expenditure in Honduras. I implemented time-series models for analyzing the principal financial indicators related to tourism.\nSkills: SPSS, Excell, forecasting, time-series."
  }
]