[
  {
    "objectID": "ABM/posts/Bayes-GLM/index.html",
    "href": "ABM/posts/Bayes-GLM/index.html",
    "title": "Una introducción al Bayesian Workflow.",
    "section": "",
    "text": "Este post da una pequeña introducción al Bayesian Workflow.\nLos métodos Bayesianos modernos se desarrollan mayoritariamente mediante ordendores. En la actualidad, múltiples algoritmos permiten aproximar las densidades a posterior en tiempo real, disminuyendo la brecha de complejidad que existía en el desarrollo y evaluación de modelos probabilistas.\nLas características mas importantes de usar métodos Bayesianos en la práctica son:\nEs importante resaltar que este Bayesian Workflow Andrew Gelman et al. (2020), es análogo al workflow Básico presentado para análisis de modelos frecuentistas. Previo a nuestra introducción de la metodología a utilizar, es necesario establecer nuestros supuestos y objetos de estudio."
  },
  {
    "objectID": "ABM/posts/Bayes-GLM/index.html#la-historia-de-siempre",
    "href": "ABM/posts/Bayes-GLM/index.html#la-historia-de-siempre",
    "title": "Una introducción al Bayesian Workflow.",
    "section": "La historia de siempre",
    "text": "La historia de siempre\nSea Y = \\{Y_1,Y_2,\\ldots,Y_n\\} una colección de variables aleatorias1 intercambiables2. Sea y = (y_1,y_2,\\ldots,y_n) el vector de datos observados (Y = y), cuya función de densidad es f(y_i|\\theta_i), y \\theta_i son desconocidos.\nEn este enfoque, \\theta_i \\in \\mathbb{R}^k es un vector de parámetros considerada aleatorio, su espacio muestral es (\\Theta,\\mathcal F, P), y su función de densidad inicial es f(\\theta).\nLa función f(\\theta) resume todos los supuestos iniciales de los parámetros desconocidos, resumiendo la incertidumbre (mide que tan incierto es el valor del parámetro para dichos datos). El objetivo es actualizar la incertidumbre mediante la nueva información obtenida (datos) del fenómeno en estudio, y por el teorema de Bayes, esta se actualiza mediante la siguiente formula:\nf(\\theta_i|y) = \\frac{f(y|\\theta_i)f(\\theta_i)}{\\int f(y|\\theta_i)f(\\theta_i)d \\theta_i} \\quad j = 1,2,\\ldots,k.\nDonde:\n\nBajo el supuesto de intercambiabilidad, f(y|\\theta_i) = \\prod_{j=1}^n f(y_j|\\theta_i).\nf(\\theta_i|y) es la posteriori de los parámetros (incertidumbre “mejorada”).\n\n\n\n\n\n\n\nNotar que:\n\n\n\n\nLa denisdad f(\\theta_i|y) = f(\\theta_i|Y=y) esta condicionada a una cantidad fija (Y=y), por lo tanto, la posterior no es aleatoria ni abstracta.\nf(y) = \\int f(y|\\theta_i)f(\\theta_i)d \\theta_i es la densidad marginal observada para Y.\nf(y) es fija, conocida, y no depende de \\theta_i, por lo tanto, se modela como una constante k.\n\n\n\nLa ecuación anterior es muy complicada de manejar y usualmente se resume como:\nf(\\theta_i|y) \\propto f(y|\\theta_i)f(\\theta_i). Donde \\propto representa la constante de proporcionalidad.\n\nDensidad Predictiva\nUna cantidad muy importante es la función predictiva a posteriori del modelo3. Sea y^* una observación nueva e independiente de la muestra y, cuya función de densidad real es f_t(y^*). Esta “nueva observación” es desconocida para los datos y se considera aleatoria, el cual se puede cuantificar mediante la siguiente ecuación:\nf(Y^*|y) = \\int f(Y^*|\\theta_i)f(\\theta_i|y) d\\theta_i, donde:\n\nf(Y^*|y) es la función predictiva a posteriori.\nY^* es la variable aleatoria que cuantifica a y^*.\nf(\\cdot|\\theta_i):\\mathbb R \\to \\mathbb R^+ para un \\theta_i fijo, es una función medible de Y^*.\nf(Y^*|\\theta_i) es una transformación de Y^*; por lo tanto, es una cantidad aleatoria nueva.\n\nEsta densidad se puede interpretar como el valor esperado a posteriori de la función generadora de datos,\nf(Y^*|y) = E_{\\theta|y}\\left[f(Y^*|\\theta_i)\\right].\nLa función predictiva es de vital importancia para realizar diagnóstico de las estimaciones obtenidas y para medir el ajuste de un modelo. El ajuste de un modelo se mide al comparar f(Y^*|\\theta_i) con su valor real f_t(y). En la práctica esta comparación tiene dos limitantes:\n\nCómo comparar funciones de densidad?\nf_t(y) siempre es desconocida.\n\nEstas limitantes se pueden sobrellevar, y de esos detalles hablaremos en las próximas secciones, por ahora, enfocarnos en la función a priori."
  },
  {
    "objectID": "ABM/posts/Bayes-GLM/index.html#tipos-de-priors",
    "href": "ABM/posts/Bayes-GLM/index.html#tipos-de-priors",
    "title": "Una introducción al Bayesian Workflow.",
    "section": "Tipos de Priors",
    "text": "Tipos de Priors\nSegún sea la función a priori definida, así serán las características de la función a posteriori. Por ejemplo, en un problema de optimización, estas densidades regularizan la verosimilitud de la muestra.\nUna correcta definición de la prior es importante para un análisis de datos objetivo e imparcial.\nEn la actualidad existen diferentes elecciones para la prior, las mas comunes son:\n\nPriors dispersas,\nPriors Objetivas,\nMaximum entropy Priors,\nPrios débiles.\n\n\nPrioris dispersas\nEste tipo de priors se caracterizan por ser distribuciones uniformes definidas en un subconjunto del espacio muestral \\Theta “muy grande”; A. Gelman et al. (2013).\n\nf(\\theta) \\propto U(a-\\varepsilon,a+\\varepsilon), \\ \\varepsilon \\to \\infty.\n\nCaracterísticas:\n\nNo proveen información externa.\nSon muy subjetivas.\nNo exploran objetivamente el espacio muestral \\Theta.\n\n\n\nPrioris objetivas\nEste tipo de priors se conocen como “no informativas”, y se caracterizan por tratar de penalizar la verosimilitud mediante el criterio de información de Fisher; Migon, Gamerman, and Louzada (2014).\n\nf(\\theta) \\propto |I(\\theta)|^{1/2}.\n Características:\n\nSon funciones de densidad impropias (No integran 1).\nProveen información pese se llamadas no informativas.\nSon invariantes a transformaciones de \\theta.\n\n\n\nMaximum entropy Priors\nEste tipo de priors se conocen como “priors de referencia”, y el objetivo es elegir la prior que sea lo mas similar posible a un posterior de referencia elegida; Bernardo and Smith (1994).\n\nf(\\theta) \\propto \\arg \\max_{f(\\theta)} H(\\theta | y),\n donde,\n\nH(\\theta | y) =-\\int f(\\theta|y)\\log f(\\theta|y)d\\theta.\n\nCaracterísticas:\n\nSon muy complicadas de computar.\nMaximizan la selección de la posterior.\nSon muy informativas, pese a ser de la misma clase que las prioris objetivas.\n\n\n\nPrioris conjugadas\nEstas priors generan posteriors con forma analítica y que pertenecen a la familia exponencial, las primeras aplicaciones surgieron a partir de este tipo de distribuciones; DeGroot and Schervish (2012).\n\nf(\\theta), \\ f(y | \\theta) \\in \\mathcal F_\\varepsilon, \\to f(\\theta|y) \\in \\mathcal F_\\varepsilon.\n\nCaracterísticas:\n\nLa posterior tiene solución analítica.\nLimitan la cantidad de modelos a utilizar.\nGarantizan un análisis objetivo de los datos, pero pueden ser muy informativas.\n\n\n\nPrioris débiles:\nNo existe una regla, formula o método para seleccionar este tipo de priors, pero se basan en elegir distribuciones que no brinden mucha información y tengan propiedades que enriquecen el análisis de modelo o la estimación del mismo; Martin, Kumar, and Lao (2021).\nCaracterísticas:\n\nproveen poca información sobre \\theta.\nregularizan la posterior.\nNo tienen forma especifica, ni método de selección.\n\nExisten múltiples estudios para cada tipo de prior estudiando los beneficios de las posteriors en un modelo en especifico, por ejemplo ver Fonseca et al. (2019). En la actualidad, existe una rama de inferencia denotada prior elicitation (Mikkola et al. (2021)) que definen algoritmos para seleccionar la mejor prior en una familia de funciones."
  },
  {
    "objectID": "ABM/posts/Bayes-GLM/index.html#estimación-de-la-posterior",
    "href": "ABM/posts/Bayes-GLM/index.html#estimación-de-la-posterior",
    "title": "Una introducción al Bayesian Workflow.",
    "section": "Estimación de la posterior",
    "text": "Estimación de la posterior\nEn la actualidad existen muchos métodos para estimación de la posterior:\n\nMonte Carlo Markov Chain (MCMC): Gibbs Sampler, Metropolis et al. (1953) y Metropolis-Hastings, Hastings (1970).\nMCMC basados en gradientes. Hamiltonean Monte Carlo (HMC) y Metropolis Adaptative Lavengian algorithm (MALA), ver Duane (1987), Hoffman and Gelman (2014), y Betancourt (2017).\nPenalized Maximum Likelihood (P-MLE): encontrar MAEP(\\theta) = \\arg \\max f(\\theta | y), el MAPE se aproxima con métodos de Quasi-Newton, en particular L-BFGS.\nApproximated Bayesian Computation (ABC): Rejection Sampler.\nVariational Inference (VI): Stochastic gradient Descent.\n\nEn la mayoría de los métodos de aproximación se obtiene una muestra \\Theta_P = \\{\\theta_1,\\theta_2,\\ldots,\\theta_S\\} de la posterior, que se puede utilizar para aproximar los estimadores puntuales e intervalos de credibilidad.\n\nEstimadores puntuales\nUn estimador puntual es el valor que minimiza una función de perdida de la posteriori, el estimador mas común es la Media a posteriori:\n\n\\hat \\theta_1 = E[\\theta | y] \\approx \\frac{1}{m}\\sum_i^m \\theta_k.\n\nOtro estimador muy utilizado es la mediana a posteriori, es bastante popular en posteriors con colas pesadas.\n\n\\hat \\theta_2 = \\text{median}(\\theta | y) \\approx \\hat q(\\Theta_P)_{0.5}.\n\nEl máximo a posterioir (MAP) es la moda de la posterior y solo se obtiene con los métodos penalized MLE y VI.\n\n\\hat \\theta_3 = \\max f(\\theta | y).\n\n\n\nIncertidumbre de los estimadores\nLa posterior del parámetro es una medida de incertidumbre en si misma, ventaja principal por la cual se prefiere inferencia Bayesiana sobre la frecuentista.\nLa forma estándar de resumir la incertidumbre es mediante los intervalos de credibilidad, estos se pueden aproximar mediante los quantiles muestrales q_\\alpha de \\Theta_P4.\nIC_{(1-\\alpha)*100\\%} = [\\hat q(\\Theta_P)_{\\alpha/2}, \\hat q(\\Theta_P)_{1-\\alpha/2}]"
  },
  {
    "objectID": "ABM/posts/Bayes-GLM/index.html#posterior-predictive-checks",
    "href": "ABM/posts/Bayes-GLM/index.html#posterior-predictive-checks",
    "title": "Una introducción al Bayesian Workflow.",
    "section": "Posterior Predictive checks",
    "text": "Posterior Predictive checks\nEstos métodos son análogos al análisis de los residuos en inferencia clásica. La idea es comparar la función predictiva f(y^*|y) con los datos obtenidos y. En la mayoría de los casos, f(y^*|y)) se aproxima con Monte-Carlo.\n\\hat f(y^*|y)  = E_{\\theta|y} [f(y^*|\\theta)] \\approx \\frac{1}{m}\\sum_{k=1}^m f(y^*|\\theta_k)\nPor lo tanto, se puede obtener una muestra de la predictiva para cada uno de los y_i observado de la forma:\n\\hat y^{(j)}_i \\sim \\frac{1}{m}\\sum_{k=1}^m f(y^*|\\theta_k), \\quad f(\\cdot|\\theta) \\text{ conocida}.\nDonde y^{(1)}_i, y^{(2)}_i, \\ldots, y^{(m)}_i es una muestra para f(y_i^*|y).\nFinalmente, los errores del modelo se estiman:\n\\hat \\varepsilon^{(j)}_i \\approx y_i - y^{(k)}_i.\n\nlog-Verosimilitud\nUn estimador muy importante para la selección de modelos es la matriz de log-verosimilitudes, esta se estima por métodos de Monte-Carlo usando una muestra de la posterior \\Theta_P, de la siguiente forma:\n\\log f(y|\\theta) = [\\log f(y_i|\\theta_j)] \\in \\mathbb R^{S \\times n}, Donde i = 1,2,\\ldots,n y j = 1,2,\\ldots,S, para el tamaño de muestra n y número de simulaciones de la posterior S. A partir de las matrices de log-verosimilitudes se puede estimar una muestra a posterior de la log-likelihood del modelo a partir de la siguiente ecuación:\n\\text{log-lik}(M)_j = -\\sum_{i=1}^n \\log f(y_i | \\theta_j). La muestra obtenida, estima la distribución a posteriori del modelo \\text{log-lik}(M). Estos valores pueden utilizarse para comparación preliminar de modelos, y se elige el modelo con criterio menor."
  },
  {
    "objectID": "ABM/posts/Bayes-GLM/index.html#selección-de-modelos",
    "href": "ABM/posts/Bayes-GLM/index.html#selección-de-modelos",
    "title": "Una introducción al Bayesian Workflow.",
    "section": "Selección de modelos",
    "text": "Selección de modelos\nSeleccionar el modelo adecuado de los datos de un conjunto de modelos M_1,M_2, \\ldots, M_k es un problema muy complicado, debido a los altos costos computacionales y complejidad de los algoritmos. En la actualidad los métodos más utilizados son:\n\nFactor de Bayes\nWatanabe-Akaike Information Criteria (WAIC).\nExpected log predictive density (elpd).\n\n\nFactores de Bayes\nLos factores de Bayes, fueron propuestos por Jeffrey (1960) y re-interpretados por Kass and Raftery (1995) para selección de modelos. Los factores de Bayes se basan en comparar las posteriors de los modelos definidos sobre los datos:\nf(M_i |y) \\propto f(y | M_i)f(M_i), Donde f(y | M_i) = f_{M_i}(y) es la verosimilitud marginal de los datos, y f(M_i) es la distribución a priori del modelo, o su importancia. Por lo tanto, el factor de Bayes es:\n\nFB = \\frac{f(M_i|y)}{f(M_j |y)} \\propto \\frac{f_{M_i}(y)f(M_i)}{f_{M_j}(y)f(M_j)},\n\nEn la práctica no tenemos importancia o favoritismo hacia un modelo entonces elegimos las priors iguales f(M_i) = f(M_j). Por lo tanto, el factor de Bayes es equivalente a la razón de verosimilitudes marginales.\nFB = \\frac{f_{M_i}(y)}{f_{M_j}(y)}.\nLa verosimilitud marginal de los datos se puede aproximar con un método de Monte-Carlo de la siguiente forma:\n\nf(y) = \\int f(y|\\theta)f(\\theta)d \\theta \\approx \\sum_{k=1}^mf(y|\\theta_k), \\quad \\theta_k \\sim f(\\theta).\n\nObservaciones:\n\nEl Factor de Bayes es sensible a modelos con priors no informativas.\nMuy complicado de estimar, y los algoritmos son inestables.\nEs perfecto para encontrar el modelo real.\n\nAproximar las verosimilitudes marginales con Monte Carlo es muy ineficiente e inestable numéricamente, otros algoritmos utilizados es muestreo por importancia y el algoritmo de Bridge-Sampling; Gronau et al. (2017).\n\n\nExpected log predictive density\nLa elpd es una divergencia entre el modelo ajustado y la densidad real de los datos que se calcula mediante la siguiente ecuación:\n\\text{elpd}(M_k|y) = - \\int\\log f(y^*|y) f_t(y)dy, Esta medida se puede aproximar usando un método de Monte-Carlo mediante la siguiente ecuación:\nelpd(M_k|y) \\approx - \\sum_{i=1}^m\\log f(y_i^*|y_i).\nEl mayor problema problema es que \\log f(y_i^*|y_i) es desconocida y se calcula a partir de la predictiva:\nf(y_i^*|y_i) = \\int f(y_i^*|\\theta)f(\\theta|y) d\\theta.\nVehtari et al. (2015) proponen hacer la estimación de la predictiva utilizando validación cruzada, cuando la forma de la predictiva es proporcional a la verosimilitud:\nf(y_i|y_{-i}) \\approx \\frac{1}{\\frac{1}{S}\\sum_{s=1}^S[f(y_i|\\theta_s)]^{-1}},\ndonde \\theta_1,\\theta_2,\\ldots,\\theta_S es una muestra de la posterior \\Theta_P; y_{-i} representa el vector original de los datos quitando la observación y_i, y f(y_i|y_{-i}) es la predictiva para y_i cuando asumimos que esta es desconocida. Por lo tanto, la elpd se aproxima con\nelpd(M_k|y) \\approx - \\sum_{i=1}^n\\log \\left[ \\frac{1}{\\frac{1}{S}\\sum_{s=1}^S[f(y_i|\\theta_s)]^{-1}} \\right]\nEl mayor problema de esta aproximación es que es muy inestable numéricamente, mucho mayor que las obtenidas en el factor de Bayes, y Vehtari et al. (2015) propone resolver esta aproximación con muestreo por importancia usando una distribución generalizada de Pareto.\nObservaciones:\n\nElige al modelo que más se acerque a la función real de los datos (f_t desconocida).\nSu estimación es con remuestreo LOO-CV y es sensible a perturbaciones o malos ajustes.\nElige al modelo que predice mejor.\n\n\n\nWatanabe-Akaike Information Criteria\nEl WAIC es un criterio de información mayormente conocido como Widely Applicable information criteria, que penalización dela log-predictiva del modelo es mediante su segundo momento.\nWAIC = -E[\\log f(y^*|y)] -n V[\\log f(y^*|y)] El criterio de información de Watanabe es asintótico al valor obtenido por la elpd, por lo tanto, puede ser aproximado con validación cruzada.\nObservaciones:\n\nse elige el modelo con menor criterio de información.\nSe puede estimar con métodos de Monte-Carlo.\nProblemático para modelos muy similares"
  },
  {
    "objectID": "ABM/posts/Logistic-GLM/index.html",
    "href": "ABM/posts/Logistic-GLM/index.html",
    "title": "Sobrevivencia en el Titanic",
    "section": "",
    "text": "Este post presenta un análisis la base Titanic usando un modelo lineal generalizado logístico.\nEl nombre del barco “Titanic” se inspira de los titanes de la mitología Griega, y pese que fue de los cruceros más grandes de su época, se hundió en su primera excursión, ocasionando la muerte de miles de sus tripulantes. Se desea crear un modelo que prediga el número de sobrevivientes en la excursión, para eso utilizaremos los registros de muerte del evento, la base de datos se divide en dos conjuntos\nLa base de datos Train contiene las siguiente variables:"
  },
  {
    "objectID": "ABM/posts/Logistic-GLM/index.html#verosimilitud",
    "href": "ABM/posts/Logistic-GLM/index.html#verosimilitud",
    "title": "Sobrevivencia en el Titanic",
    "section": "Verosimilitud",
    "text": "Verosimilitud\nLa variable survival es una variable indicadora que representa si el pasajero abordo sobrevivió el hundimiento del Titanic. Dado que es una v.a. discreta y dicotómica se puede modelar con una distribución de Bernoulli.\ny_i \\sim Bernoulli(p), \\quad y_i = 0,1. \\quad \\& \\quad f(y) = p^y(1-p)^{(1-y)}. Donde p representa la probabilidad de éxito (éxito := 1), que indica que el paciente sobrevivió.\nPara este tipo de modelos se utilizan GLMs Logísticos, cuya estructura se presenta en la siguiente ecuación:\ny_i \\sim \\text{Bernoulli}(p_i),\\quad g(p) = \\text{logit}(p_i), \\text{ y } p_i = \\frac{1}{1 + e^{-\\beta X_i}}. Donde:\n\ng:\\mathbb R \\to \\mathbb R, es la función logit g(x) = \\log\\left(\\frac{p}{1-p}\\right).\ng^{-1}(x) = \\frac{1}{1 + e^{-x}} es la función logística.\nX son las covariables.\n\nFinalmente, realizamos un gráfico de correlaciones para identificar las interacciones lineales entre variables.\n\nggpairs(Train[,-1])\n\n\n\n\nFigure 1: Gráfico de pares. La diagonal principal muestra histogramas densidades de cada una de las variables. La parte superior muestra el coeficiente de correlación entre dos variables, fila y columna. La parte inferior muestra un gráfico de dispersión entre dos variables.\n\n\n\n\nLa Figure 1 es poco informativa debido que múltiples variables, incluida la de interés, son variables discretas. Una forma alternativa de medir la dispersión en v.a.d. es usar gráficos de barras compuestos o gráfico de bombones. Para medir correlación un indicador no paramétrico equivalente, es el coeficiente de Kendall."
  },
  {
    "objectID": "ABM/posts/Logistic-GLM/index.html#ajuste-del-glm-logístico",
    "href": "ABM/posts/Logistic-GLM/index.html#ajuste-del-glm-logístico",
    "title": "Sobrevivencia en el Titanic",
    "section": "Ajuste del GLM Logístico",
    "text": "Ajuste del GLM Logístico\nAjustamos el modelo GLM logístico completo que consiste en usar todas las variables, y revisamos el ajuste e inferencia de los parámetros.\n\nm1  = glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare,\n            data = Train, family = \"binomial\")\n\nsummary(m1)\n\n\nCall:\nglm(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + \n    Fare, family = \"binomial\", data = Train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7953  -0.6476  -0.3847   0.6271   2.4433  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  5.389003   0.603734   8.926  < 2e-16 ***\nPclass      -1.242249   0.163191  -7.612 2.69e-14 ***\nSexmale     -2.634845   0.219609 -11.998  < 2e-16 ***\nAge         -0.043953   0.008179  -5.374 7.70e-08 ***\nSibSp       -0.375755   0.127361  -2.950  0.00317 ** \nParch       -0.061937   0.122925  -0.504  0.61436    \nFare         0.002160   0.002493   0.866  0.38627    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 964.52  on 713  degrees of freedom\nResidual deviance: 635.81  on 707  degrees of freedom\n  (177 observations deleted due to missingness)\nAIC: 649.81\n\nNumber of Fisher Scoring iterations: 5\n\n\nEl modelo completo da una impresión con buenos resultados, todas las variables excepto el precio de boletos (Fare) y el número de Padres/hijos (parch) resultaron significativas; pero los residuos no están centrados en cero, por ende no cumplen los supuestos iniciales.\nEl siguiente código genera una muestra Bootstrap para los parámetros del modelo M_1.\n\n\nCode\nglm_boots = function(dat,B = 1000){\n  n = dim(dat)[1]\n  est = NULL\n  for (i in 1:B) {\n    si = sample(x = 1:n,size = n,replace = TRUE)\n    dsi = dat[si,]\n    mli = glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare,\n              data = dsi,family = binomial)\n    ci = as.array(mli$coefficients)\n    est = rbind(est,ci)\n  }\n  # Estética\n  return(est)\n}\n\n\nObtenemos una muestra Bootstrap para los estimadores \\hat \\beta de tamaño B = 2,000 repeticiones.\n\nbtp = glm_boots(dat = Train,B = 2000)\n\ncolor_scheme_set(\"green\")\nmcmc_dens(btp)+labs(title=\"Distribución muestral de los estimadores, GLM Logístico\",\n                    subtitle =\"Bootstrap B = 2,000 iteraciones\")\n\n\n\n\nFigure 2: Gráfico de densidades. Cada densidad representa la distribución muestral aproximada para cada uno de los estimadores usando un Bootstrap de B = 2,000 iteraciones.\n\n\n\n\nLos intervalos de confianza al 95% son:\n\nx = apply(btp,MARGIN = 2, FUN = quantile, probs = c(0.025,0.5,0.975)) \n\n# Estética\nx = data.frame( t(x) )\nx$pars = c(\"intercepto\",\"Pclass\",\"Sexmal\",\"Age\",\"SibSp\",\"Parch\",\"Fare\")\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"parámetros\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\nTable 1:  Intervalos de confianza al 95%, obtenidos a partir de una muestra bootstrap de B = 2,000 iteraciones parámetrosq2.5%Medianq97.5%intercepto4.3342606755.4396350936.745395492Pclass-1.639020256-1.248013748-0.920773130Sexmal-3.103889872-2.670659414-2.236386690Age-0.061857970-0.044474501-0.028015033SibSp-0.650111165-0.388576745-0.140536577Parch-0.308640960-0.0594306060.181213922Fare-0.0032360970.0023711310.008264565\n\n\n\nLa Figure 2 muestra la distribución muestral de los estimadores del modelo, y la Table 1 muestra los intervalos de confianza.El efecto de las variables Fare y Parxh está concentrado en cero, por lo tanto, se deberá considerar un GLM logístico reducido.\nLos residuos no son una medida correcta para evaluar el ajuste del modelo. Esto se debe a que el modelo predice de forma continua valores en el intervalo unitario I = [0,1] y los datos son los enteros en la clausura de I. Una forma adecuada de visualizar los residuos es usando la matriz de confusión, esta es una matriz en \\mathbb{R}^{2 \\times 2} que presenta el ajuste del modelo.\n\nCM = \\begin{pmatrix}\n    P & FP\\\\\n    FN & N\n\\end{pmatrix}\n Donde:\n\nP representa los valores predichos correctamente como positivos (1).\nN representa los valores predichos correctamente como negativos (0).\nFP representan los valores falsos positivos, \\hat y = 1 cuando y = 0.\nFN representan los valores falsos negativos, \\hat y = 0 cuando y = 1.\n\n\npred1 = predict(m1,Train[,-c(1:2)],type = \"response\")\npred1 =  ifelse(pred1 > 0.5, 1, 0)\n   \nx = table(pred1, Train$Survived)\nx = round(prop.table(x)*100,2)\nx\n\n     \npred1     0     1\n    0 50.98 11.20\n    1  8.40 29.41\n\n\nUna medida importante es la precisión (“Accuracy”) del modelo, que es el porcentaje de aciertos del modelo. Que se calcula como la suma de las diagonales en la matriz de confusión. El accuracy para el modelo M_1 es:\n\nAccuracy = sum(diag(x))\nAccuracy\n\n[1] 80.39"
  },
  {
    "objectID": "ABM/posts/Logistic-GLM/index.html#selección-de-modelos-10-fold-cv",
    "href": "ABM/posts/Logistic-GLM/index.html#selección-de-modelos-10-fold-cv",
    "title": "Sobrevivencia en el Titanic",
    "section": "Selección de modelos, 10-fold-CV",
    "text": "Selección de modelos, 10-fold-CV\nPara seleccionar el mejor modelo usaremos validación cruzada, 10-fold, esto implica que ajustaremos diez veces cada modelo, evaluando la precisión del modelo. Los modelos que se consideraran son los siguientes:\n\nM_1: Modelo de logístico completo\nM_2: Modelo logístico reducido sin la variable Parch.\nM_3: Modelo logístico reducido sin variables Parch y Fare.\n\nEl siguiente código presenta una función para realizar k-fold-CV para cualquier valor de k. En caso de querer añadir otros modelos o criterios, la función deberá ser modificada.\n\n\nCode\nkfold = function(df,k){\n  # Generar la particion\n  kfld = createFolds(df[,1],k = k)\n  mat = NULL\n  \n  for (i in 1:k) {\n    # separar los datos en conjuntos de prueba y entrenamiento\n    dfE= df[-kfld[[i]],]\n    dfP = df[kfld[[i]],]\n    # Ajustar los modelos\n    m1  = glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare,\n              data = dfE,family = binomial)\n    m2 = glm(Survived ~ Pclass + Sex + Age + SibSp + Fare,\n              data = dfE,family = binomial)\n    m3  = glm(Survived ~ Pclass + Sex + Age + SibSp,\n              data = dfE,family = binomial)\n    \n    p1  = predict(m1,dfP,type = \"response\")\n    p1 =  ifelse(p1 > 0.5, 1, 0)\n    p2 = predict(m2,dfP,type = \"response\")\n    p2 =  ifelse(p2 > 0.5, 1, 0)\n    p3  = predict(m3,dfP,type = \"response\")\n    p3 =  ifelse(p3 > 0.5, 1, 0)\n\n    Accuracy = c(\n      sum(diag(round(prop.table(table(p1, dfP[,2]))*100,2))),\n      sum(diag(round(prop.table(table(p2, dfP[,2]))*100,2))),\n      sum(diag(round(prop.table(table(p3, dfP[,2]))*100,2)))\n    )\n\n    # Unir los datos\n    mat = rbind(mat,Accuracy) \n  }\n  colnames(mat) = c(\"M1\",\"M2\",\"M3\")\n  row.names(mat) = NULL\n  return(mat)\n}\n\n\nTable 2 presenta los resultados obtenidos al realizar 10-fold-cv, el modelo M_3 es el que presenta la mejor precisión de los tres modelos evaluados.\n\nrst = kfold(df = na.exclude(Train),k = 10)\nx = t(apply(rst,MARGIN = 2,FUN = \"quantile\",probs = c(0.025,0.5,0.975)))\n\n# Estética\nx = data.frame(x)\nx$pars =  c(\"M1\", \"M2\", \"M3\")\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"Accuracy\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\nTable 2:  Precisión de los modelos. Mediante Validación cruzada. Accuracyq2.5%Medianq97.5%M173.76380.43085.48725M273.76380.55585.44000M373.76380.55585.44000\n\n\n\nTable 2 compara los tres modelos, usando la medida de precisión, y el modelo reducido M_3 tiene resultados ligeramente mejores. Finalmente, se predice en el conjunto de prueba, la Figure 3 presenta las predicciones realizadas por el modelo M_3 para 332 pasajeros, donde la mayoría de ellos no sobreviven al hundimiento del barco.\n\nm3 = glm(Survived ~ Pclass + Sex + Age + SibSp,\n              data = Train,family = binomial)\n\np3  = predict(m3,Test,type = \"response\")\np3 =  ifelse(na.exclude(p3) > 0.5, 1, 0)\np3 = data.frame(p3)\n\nggplot(p3, aes(x=as.factor(p3), fill=as.factor(p3) )) + \n  geom_bar( ) + scale_fill_hue(c = 40) +\n  labs(title = \"Predicciones de la sobrevivencia de los pasajeros, Titanic\",\n       subtitle = \"Conjunto de Prueba\",\n       x = \"Sobrevivencia\",y = \"Conteo\")+theme(legend.position=\"none\")\n\n\n\n\nFigure 3: Gráfico de predicciones. La barra roja presenta el número de pasajeros que no sobrevivieron al hundimiento del Titanic."
  },
  {
    "objectID": "ABM/posts/ABR/index.html",
    "href": "ABM/posts/ABR/index.html",
    "title": "Applied Bayesian Regression",
    "section": "",
    "text": "Este post presenta el contenido, direcciones y detalles de “Applied Bayesian Regression.”\nLa segunda parte del curso se impartirá regresión Bayesiana aplicada, el objetivo principal es que el estudiante pueda resolver un problema de regresión con un conjunto de datos reales mediante un ordenador, independiente del enfoque de inferencia a utilizar.\nEl material de la clase se encuentra en el repositorio Applied-Bayesian-Regression. Este presenta el contenido en cuadernos de la clase de “Modelos lineales” de la ’Maestría en Matemática, UNAH”, tercer periodo del año 2022. Los archivos más importantes son:\nLa clase se imparte los Lunes, Martes y Jueves a las 17:00 horas (GMT -6), Para acceder al enlace zoom de la clase presione aquí."
  },
  {
    "objectID": "ABM/posts/ABR/index.html#contenido",
    "href": "ABM/posts/ABR/index.html#contenido",
    "title": "Applied Bayesian Regression",
    "section": "Contenido",
    "text": "Contenido\nEl contenido para el resto del curso es:\n\nRegresión aplicada\n\nVerosimilitud y función de enlace.\nL-BFGS algorithm.\nIntervalos de confianza, Jackniffe y Bootstrap.\nAnálisis de residuos, ANOVA y R^2 ajustado.\nselección del modelos (BIC, RMSE, MAPE, CV)\n\nRepaso de inferencia Bayesiana y Bayesian workflow\n\nPrior, likelihood, Posterior\nMCMC\nPredictive distribution\nPosterior predictive checks\nBayes factor, ELPD, LOO-CV.\n\nBayesian Regression\n\nBayesian GLMs, normal, Binomial, Poisson and Negative Binomial regressions\nRegularized priors (R2-D2, Horseshoe, Spike Lab)\nGaussian process regression."
  },
  {
    "objectID": "ABM/posts/ABR/index.html#material",
    "href": "ABM/posts/ABR/index.html#material",
    "title": "Applied Bayesian Regression",
    "section": "Material",
    "text": "Material\nEl material de la clase se extrae de 3 libros, varios artículos y diferentes paquetes de R y Python estos son libres y se encuentran en formato digital en la web.\n\nLibros\n\nBayes Rules! An Introduction to Applied Bayesian Modeling. Johnson, Ott and Dogucu, (2021).\nBeyond Multiple Linear Regression Applied Generalized Linear Models and Multilevel Models in R. Roback and Legler (2021).\nBayesian Modeling and Computation in Python. Martin, Kumar, and Lao (2021).\n\n\n\nArtículos principales\n\nBayesian Regression Using a Prior on the Model Fit: The R2-D2 shrinkage Prior. Zhang et al. (2022)\nHandling Sparsity via the Horseshoe. Carvalho, Polson, and Scott (2009)\nBayesian Variable Selection in Linear Regression. Mitchell and Beauchamp (1988)\n\n\n\nPaquetes\nLos lenguajes de programación a usar son R y Python.\n\n\nR core team\n\nProbabilistic Programming Language: Stan mc-stan.\npaquetes:\n\nrstanarm, paquete para ajustar modelos lineales.\nbayesplot, visualización de posterioris.\nloo seleccion de modelos.\n\n\n\n\nPython\n\nProbabilistic Programming Language: PyMC.\npaquetes:\n\nBambi, Ajustar modelos lineales.\nArviZ, visualización de datos y selección de modelos con LOO."
  },
  {
    "objectID": "ABM/posts/GAM/index.html",
    "href": "ABM/posts/GAM/index.html",
    "title": "Sobrevivencia en el Titanic, parte 2",
    "section": "",
    "text": "Este post presenta un análisis la base Titanic usando un modelo aditivo generalizado logístico.\nLos modelos aditivos generalizados (GAMs) relacionan forma no lineal al conjunto de v.a. independientes Z = \\{Z_1,Z_2,\\ldots,Z_n\\}, tal que Z_i = (Y_i,X_i) \\in \\mathbb R^{d+1}. La colección Z sigue un GAM si:\nY_i \\sim \\mathscr{F}_\\varepsilon (\\theta_i), \\quad \\text{y } g(\\theta_i) = \\sum_{s = 1}^mf_s(X_i), Donde:\nf_s(X) \\approx \\sum^K_{k = 1}\\beta_{k,s} b_{k,s}(X),\nDado que\nB_s(X) = [b_{1,s}(X),b_{2,s}(X),\\ldots,b_{K,s}(X)], es una base, entonces los b_{k,s}(X) son una colección linealmente independiente en un espacio de funciones (L_2). Este resultado implica que estimar la función no lineal f_s(X) es equivalente a resolver el problema de regresión lineal:\n\\hat f_s(X) = B_s(X) \\hat \\beta. Este tipo de modelos tiene dos limitantes, el primero es seleccionar una base adecuada, y segundo elegir el número de óptimo de elementos en la base K. Si K es muy grande el modelo sobre-ajusta los datos (overfitting). Una solución es penalizar el suavizado con:\nP(f) = \\int \\partial^2f^2(x) dx. Por lo tanto, el problema de optimización se escribe como:\n\\hat \\beta_s = \\arg \\min_\\beta ||g^{-1}(y) - f_s(X)||^2 + \\lambda P(f), \\hat \\beta_s = \\arg \\min_\\beta || g^{-1}(y) - B_s(X)\\beta ||^2 + \\lambda \\beta^tS\\beta.\nDonde S = \\int B_s^t(X)B_s(X) dx, es la penalización. Note que el problema de optimización anterior es equivalente a resolver un problema de regresión lineal penalizado, que a su vez puede ser re-escrito como un problema de regresión lineal completo."
  },
  {
    "objectID": "ABM/posts/GAM/index.html#ejemplo-mcycle-dataset",
    "href": "ABM/posts/GAM/index.html#ejemplo-mcycle-dataset",
    "title": "Sobrevivencia en el Titanic, parte 2",
    "section": "Ejemplo: mcycle dataset",
    "text": "Ejemplo: mcycle dataset\n\n\nCode\nlibrary(mgcv)\nlibrary(caret)\nlibrary(tidymv)\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(flextable)\nlibrary(bayesplot)\n\nbayesplot_theme_set(theme_grey())\ndata(\"mcycle\",package = \"MASS\")\nload(\"Titanic.RData\")\n\n\nLa base de datos mcycles presenta una serie de mediciones de la aceleración de cabezas en un accidente motorizado simulado para probar la resistencia al impacto en cascos. La base de datos contiene las siguiente variables:\n\naccel: La aceleración obtenida al momento de impacto. (Dependiente)\ntimes: El tiempo previo al impacto. (ms)\n\n\nggplot(mcycle,aes(times, accel))+\n  geom_point()+\n  labs(title = \"Aceleración de cabezas al momento de impacto\",\n       subtitle = \"Accidentes motorizados simulados\",\n       y = \"aceleracion (g)\",x  = \"tiempo (ms)\")\n\n\n\n\nFigure 1: Gráfico de dispersión de las simulaciones.\n\n\n\n\nPara medir la relación de la aceleración obtenida de los accidentes utilizaremos un GAM normal tal que:\naccel_i \\sim N(\\mu_i,\\sigma^2), \\quad  g(\\mu_i) = \\mu_i, \\text{ y } \\mu_i = f(times_i). Donde f(times) = \\sum^k_{t = 1}\\beta_t b_t(times) representa aproximación por base Splines para la media del modelo. El siguiente código limpia la base de datos para obtener las variables de interés\n\nm1 = gam(accel ~ s(times,k = 15,bs = \"fs\",m = 2),data = mcycle)\nsummary(m1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\naccel ~ s(times, k = 15, bs = \"fs\", m = 2)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -25.546      1.965     -13   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df     F p-value    \ns(times) 10.44  12.18 38.07  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.78   Deviance explained = 79.8%\nGCV = 561.61  Scale est. = 513.3     n = 133\n\n\nEl resumen del modelo presenta un análisis de significancia para el vector de parámetros \\hat \\beta, que en este caso es de dimensión d = 15, y que debido al TLC se distribuye normal multivariada. El siguiente código genera una muestra Bootstrap para cada uno de los parámetros del modelo M_1.\n\n\nCode\ngam_boots = function(dat,B = 2000){\n  n = dim(dat)[1]\n  est = NULL\n  for (i in 1:B) {\n    si = sample(x = 1:n,size = n,replace = TRUE)\n    mli = gam(accel[si] ~ s(times[si],k = 15,bs = \"fs\",m = 2),data = dat)\n    ci = as.array(mli$coefficients)\n    est = rbind(est,ci)\n  }\n    # Estética\n  cn = dim(est)[2]-1\n  colnames(est) = c(\"intercepto\",paste0(\"beta\",1:cn))\n  return(est)\n}\n\n\nObtenemos una muestra Bootstrap para los estimadores \\hat \\beta de tamaño B = 2,000 repeticiones.\n\nbtp = gam_boots(dat = mcycle,B = 2000)\n\ncolor_scheme_set(\"viridis\")\n\nmcmc_dens(btp)+\n  labs(title = \"Distribución muestral de los estimadores, GAM\",\n       subtitle =\"Bootstrap B = 2,000 iteraciones\")\n\n\n\n\nFigure 2: Gráfico de densidades. Cada densidad representa la distribución muestral aproximada para cada uno de los estimadores usando un Bootstrap de B = 2,000 iteraciones.\n\n\n\n\nLos intervalos de confianza al 95% son:\n\nx = apply(btp,MARGIN = 2, FUN = quantile, probs = c(0.025,0.5,0.975)) \n\n# Estética\nx = data.frame( t(x) )\nx$pars = c(\"intercepto\",paste0(\"beta\",1:14))\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"parámetros\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\nTable 1:  Intervalos de confianza al 95%, obtenidos a partir de una muestra bootstrap de B = 2,000 iteraciones parámetrosq2.5%Medianq97.5%intercepto-33.74242-25.51390977-17.59773beta1-86.43808-50.5377510686.20654beta2-56.22651-3.2892616048.04959beta3-92.8097898.00417475124.52621beta4-103.00675-53.78880623-6.37414beta5-80.28676-28.9892627477.06557beta6-89.0496538.7992333090.37534beta7-26.02682-0.0611479623.77885beta8-31.465330.4999307432.48495beta9-29.53657-2.7217765820.40368beta10-36.235200.5460341234.96996beta11-20.275821.4375412324.66445beta12-31.467001.8339684034.85059beta13-98.04408-1.4656809899.90742beta14-15.7301310.9634950942.98066\n\n\n\nLos intervalos de confianza revelan mayor información a lo obtenido por la prueba-t, el análisis de incertidumbre revela que parámetros desde beta7 hasta beta14 son no significativos, y otros parámetros como beta1 sus intervalos de confianza son no significativos pero sus densidades muestran que cero no contiene probabilidad.\nEste análisis revela que el número de nodos en la base es muy grande, por lo tanto, se deberá probar con un modelo más pequeño. Analizamos los residuos del modelo, el siguiente código muestra están centrados en cero pero con alta dispersión.\n\nsummary(m1$residuals)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-76.8753 -12.3859  -0.6566   0.0000  12.5556  50.5333 \n\n\nFinalmente, ajustamos un modelo más pequeño, el modelo M_2 es un modelo con base splines tal que el numero de nodos se estimo con validación cruzada generalizada (GCV), resultando con k = 10 nodos.\n\nm2 = gam(accel ~ s(times),data = mcycle)\n\npred1 = predict_gam(m1)\npred2 = predict_gam(m2)\n\npred = rbind(pred1,pred2)\npred$model = sort(rep(c(\"m1\",\"m2\"),50))\n\nggplot(pred,aes(times, fit))+\n  geom_point(data = mcycle,aes(x = times,y = accel))+\n  geom_smooth_ci(group = model,ci_alpha = 0.4)+\n  labs(title = \"Ajuste de los modelos GAMs\",\n       subtitle = \"Accidentes motorizados simulados\",\n       y = \"aceleracion (g)\",x  = \"tiempo (ms)\")\n\n\n\n\nFigure 3: Gráfico de ajuste. Se muestra el ajuste de ambos modelos, estos estiman una curva suave para ambos datos, con la limitante de que los intervalos predictivos no se ajustan a los valores atípicos.\n\n\n\n\nDado que en este modelo queremos encontrar el mejor ajuste, usar un criterio de información como el AIC es suficiente y necesario. Table 2 compara ambos modelos donde el modelo M_2 que contiene menos nodos presenta un mejor ajuste.\n\nx = AIC(m1,m2)\n\n# Estética\nx = data.frame(x)\nx$modelo = c(\"M1\",\"M2\")\ncolnames(x) = c(\"df\",\"AIC\",\"Modelo\")\n\nft = flextable(x[c(3,1,2)])\nautofit(ft)\n\n\n\nTable 2:  Comparación de modelos bajo el criterio de información de Akaike ModelodfAICM112.441321,220.390M210.693311,216.889"
  },
  {
    "objectID": "ABM/posts/GAM/index.html#sobrevivencia-en-el-titanic",
    "href": "ABM/posts/GAM/index.html#sobrevivencia-en-el-titanic",
    "title": "Sobrevivencia en el Titanic, parte 2",
    "section": "Sobrevivencia en el Titanic",
    "text": "Sobrevivencia en el Titanic\nRegresamos al análisis de sobrevivencia del Titanic, la base de datos contiene 891 registros y 9 variables, en el trabajo anterior se estimo un modelo GLM logístico para predecir la sobrevivencia de los pasajeros. El modelo seleccionado descartó variables como Fare y Parch, resultando en el modelo:\nM2: survival_i \\sim \\text{Bernoulli}(p_i), \\quad p_i = \\frac{1}{1 + e^{-\\beta X_i}}; y \\quad X_i = [Pclass, Sex, Age, SibSp]. El análisis realizado no contempla relaciones no lineales entre Fare y survival. Por lo tanto, re-evaluamos la interacción no lineal con las variables continuas Age y Fare. El nuevo modelo es:\nM3: survival \\sim \\text{Bernoulli}(p_i), \\quad p_i = \\frac{1}{1 + e^{f_1(X_i)+f_2(X_k)}}. Donde:\n\ng(p) = \\text{logit}(p_i), es la función de enlace.\ng(p) = f_1(X_i) + f_2(X_k). Las covariables interactuan de forma aditiva.\nf_1(X_i) = \\beta_1*[Pclass, Sex, SibSp], es un funcional lineal.\nf_2(X_K) = f(Age,Fare) \\approx \\sum_{k=1}^K \\beta_2 b(Age, Fare), es un funcional no lineal."
  },
  {
    "objectID": "ABM/posts/GAM/index.html#ajuste-del-gam-logístico",
    "href": "ABM/posts/GAM/index.html#ajuste-del-gam-logístico",
    "title": "Sobrevivencia en el Titanic, parte 2",
    "section": "Ajuste del GAM Logístico",
    "text": "Ajuste del GAM Logístico\nAjustamos el modelo GAM logístico propuesto,y revisamos el ajuste e inferencia de los parámetros.\n\nm3  = gam(Survived ~ Pclass + Sex + SibSp + s(Fare,Age),\n            data = Train, family = \"binomial\")\nsummary(m3)\n\n\nFamily: binomial \nLink function: logit \n\nFormula:\nSurvived ~ Pclass + Sex + SibSp + s(Fare, Age)\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   4.1641     0.5141   8.099 5.53e-16 ***\nPclass       -1.2076     0.2123  -5.689 1.28e-08 ***\nSexmale      -2.6842     0.2261 -11.869  < 2e-16 ***\nSibSp        -0.5303     0.1605  -3.305 0.000951 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df Chi.sq p-value   \ns(Fare,Age) 16.65  21.44  46.34 0.00103 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.437   Deviance explained = 37.7%\nUBRE = -0.10016  Scale est. = 1         n = 714\n\n\nEl modelo parece ser adecuado, todos los parámetros son significativos y la interaccion no lineal parece brindar buenos resultados. El siguiente código genera una muestra Bootstrap para cada uno de los parámetros del modelo M_1.\n\n\nCode\ngam_boots = function(dat,B = 2000){\n  n = dim(dat)[1]\n  est = NULL\n  for (i in 1:B) {\n    si = sample(x = 1:n,size = n,replace = TRUE)\n    mli = gam(Survived[si] ~ Pclass[si] + Sex[si] + SibSp[si] + s(Fare[si],Age[si]),\n            data = dat, family = \"binomial\")\n    ci = as.array(mli$coefficients)\n    est = rbind(est,ci)\n  }\n    # Estética\n  cn = dim(est)[2]-4\n  colnames(est) = c(\"intercepto\",\"Pclass\",\"Sex\",\"SibSp\",paste0(\"beta\",1:cn))\n  return(est)\n}\n\n\nObtenemos una muestra Bootstrap para los estimadores \\hat \\beta de tamaño B = 2,000 repeticiones.\n\nbtp = gam_boots(dat = na.exclude(Train),B = 5000)\n\ncolor_scheme_set(\"viridis\")\n\nmcmc_dens(btp)+\n  labs(title = \"Distribución muestral de los estimadores, GAM\",\n       subtitle =\"Bootstrap B = 2,000 iteraciones\")\n\n\n\n\n\n\nFigure 4: Gráfico de densidades. Cada densidad representa la distribución muestral aproximada para cada uno de los estimadores usando un Bootstrap de B = 2,000 iteraciones.\n\n\n\n\nLa Figure 2 muestra la distribución muestral de los estimadores del modelo, se observa que los efectos de la componente no lineal son muy dispersos y centrados en cero. Existe la posibilidad que el efecto no lineal sea no significativo.\nPara evaluar el ajuste del modelo calculamos la matriz de confusión y la precisión del modelo.\n\npred1 = predict (m3,Train[,-c(1:2)],type = \"response\")\npred1 =  ifelse(pred1 > 0.5, 1, 0)\n   \nx = table(pred1, Train$Survived)\nx = round(prop.table(x)*100,2)\nx\n\n     \npred1     0     1\n    0 51.26 10.50\n    1  8.12 30.11\n\n\nLa precisión para el modelo M_3 es:\n\nAccuracy = sum(diag(x))\nAccuracy\n\n[1] 81.37"
  },
  {
    "objectID": "ABM/posts/GAM/index.html#selección-de-modelos-10-fold-cv",
    "href": "ABM/posts/GAM/index.html#selección-de-modelos-10-fold-cv",
    "title": "Sobrevivencia en el Titanic, parte 2",
    "section": "Selección de modelos, 10-fold-CV",
    "text": "Selección de modelos, 10-fold-CV\nPara seleccionar el mejor modelo usaremos validación cruzada, 10-fold, esto implica que ajustaremos diez veces cada modelo, evaluando la precisión del modelo. Los modelos que se consideraran son los siguientes:\n\nM_2: GLM logístico reducido sin variables Parch y Fare.\nM_3 GAM logístico con variables lineales Pclass, Sex, y SibSp. Componente no lineal Age y Fare\n\nEl siguiente código presenta una función para realizar k-fold-CV para cualquier valor de k. En caso de querer añadir otros modelos o criterios, la función deberá ser modificada.\n\n\nCode\nkfold = function(df,k){\n  # Generar la particion\n  kfld = createFolds(df[,1],k = k)\n  mat = NULL\n  \n  for (i in 1:k) {\n    # separar los datos en conjuntos de prueba y entrenamiento\n    dfE= df[-kfld[[i]],]\n    dfP = df[kfld[[i]],]\n    # Ajustar los modelos\n    m2  = glm(Survived ~ Pclass + Sex + Age + SibSp,\n              data = dfE,family = binomial)\n    m3  = gam(Survived ~ Pclass + Sex + SibSp + s(Fare,Age),\n            data = dfE, family = \"binomial\")\n\n    p2 = predict(m2,dfP,type = \"response\")\n    p2 =  ifelse(p2 > 0.5, 1, 0)\n    p3  = predict(m3,dfP,type = \"response\")\n    p3 =  ifelse(p3 > 0.5, 1, 0)\n\n    Accuracy = c(\n      sum(diag(round(prop.table(table(p2, dfP[,2]))*100,2))),\n      sum(diag(round(prop.table(table(p3, dfP[,2]))*100,2)))\n    )\n\n    # Unir los datos\n    mat = rbind(mat,Accuracy) \n  }\n  colnames(mat) = c(\"Accuracy1\",\"Accuracy2\")\n  row.names(mat) = NULL\n  return(mat)\n}\n\n\nTable 3 presenta los resultados obtenidos al realizar 10-fold-cv, el modelo M_3 es el que presenta la mejor precisión de los dos modelos evaluados.\n\nrst = kfold(df = na.exclude(Train),k = 10)\nx = t(apply(rst,MARGIN = 2,FUN = \"quantile\",probs = c(0.025,0.5,0.975)))\n\n# Estética\nx = data.frame(x)\nx$pars =  c(\"Accuracy2\", \"Accuracy3\")\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"Criterio\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\nTable 3:  Precisión de los modelos. Mediante Validación cruzada. Criterioq2.5%Medianq97.5%Accuracy272.4645081.68587.18725Accuracy372.5432580.41587.18725"
  },
  {
    "objectID": "ABM/posts/NB-GLM/index.html",
    "href": "ABM/posts/NB-GLM/index.html",
    "title": "Tratamiento para cucarachas",
    "section": "",
    "text": "Este post presenta un análisis la base roaches usando un modelo lineal generalizado de conteo.\nSe desea establecer la eficacia de cierto pesticida para el reducir el número de cucarachas en apartamentos urbanos. El tratamiento se aplicó a 158 de 262 apartamentos, y es el número de cucarachas atrapadas después de aplicar dicho tratamiento.\nLos resultados del estudio se almacenan en la base de datos roaches, que contiene las siguiente variables."
  },
  {
    "objectID": "ABM/posts/NB-GLM/index.html#verosimilitud",
    "href": "ABM/posts/NB-GLM/index.html#verosimilitud",
    "title": "Tratamiento para cucarachas",
    "section": "Verosimilitud",
    "text": "Verosimilitud\nLa variable y representa el número de cucarachas registradas al finalizar el tratamiento, dicha característica es representada por una v.a. discreta y positiva, y dicha característica debe ser considerada en el modelo.\n\nmcmc_dens(roaches,pars = c(\"y\",\"roach1\"))+\n  labs(title = \"Gráfico de densidades para el número de cucarachas\",\n       subtitle = \"Pre|Post tratamiento\")\n\n\n\n\nFigure 1: El gráfico izquierdo presenta las densidades de las dos variables de interés, el número de cucarachas previo y post exposición al tratamiento. Ambas variables muestran medias centradas en cero pero con colas bastante pesadas.\n\n\n\n\nFigure 1 muestra que ambas variables y y roach1 son positivas, con colas derechas muy pesadas, para este tipo de datos existen dos alternativas:\n\nModelar los datos con una distribución \\log N(\\mu,\\sigma^2).\nModelar los datos con GLMs de conteo\n\nEn GLMs de conteo, la distribución mas popular debido a su simpleza es Poisson pero con una fuerte limitación que los datos poseen media y varianza iguales. Para medir los efectos del tratamiento mediante un GLM de conteo de Poisson definimos la verosimilitud de tal forma que:\ny_i \\sim \\text{Poisson}(\\mu_i),\\quad g(\\mu_i) = \\log(\\mu_i), \\text{ y } \\mu_i = \\mu_0e^{\\beta X_i}. Donde:\n\n\\mu_0 se le conoce como la información previo a la exposición.\ng:\\mathbb R \\to \\mathbb R, es la función de enlace logarítmica g(x) = \\log x\ny X son las covariables.\n\nEn un modelo log-normal, asumimos que los datos en escala logarítmica siguen un modelo normal\n\\log y_i\\sim N(\\mu_i,\\sigma^2), \\quad  g(\\mu_i) = \\mu_i, \\text{ y } \\mu_i = \\beta X_i. Es importante tener en cuenta que la función logarítmica es convexa, por ende no se puede aplicar la transformación inversa para obtener las predicciones en las escalas originales. Recordar que si y \\sim \\log N(\\mu,\\sigma^2) entonces:\nE[y] = e^{\\mu +1/2\\sigma}. Finalmente realizamos un gráfico de correlaciones para identificar las interacciones lineales entre variables.\n\nggpairs(roaches )\n\n\n\n\nFigure 2: Gráfico de pares. La diagonal principal muestra histogramas densidades de cada una de las variables. La parte superior muestra el coeficiente de correlación entre dos variables, fila y columna. La parte inferior muestra un gráfico de dispersión entre dos variables.\n\n\n\n\nFigure 2 muestra resultados anti-intuitivos, se esperaría una alta correlación entre las variables y y roach1, dado que ambas miden la misma información pero en tiempos diferentes. Dado la poca correlación entre las variables consideramos un modelo completo que incluya todas las interacciones en el modelo."
  },
  {
    "objectID": "ABM/posts/NB-GLM/index.html#ajuste-del-modelo-de-conteo-de-poisson",
    "href": "ABM/posts/NB-GLM/index.html#ajuste-del-modelo-de-conteo-de-poisson",
    "title": "Tratamiento para cucarachas",
    "section": "Ajuste del modelo de Conteo de Poisson",
    "text": "Ajuste del modelo de Conteo de Poisson\nAjustamos el modelo GLM de conteo completo que consiste en usar todas las variables, y revisamos el ajuste e inferencia de los parámetros.\n\nm1  = glm(y ~ roach1 + treatment + senior, offset = log(exposure2),\n            data = roaches, family = poisson)\n\nsummary(m1)\n\n\nCall:\nglm(formula = y ~ roach1 + treatment + senior, family = poisson, \n    data = roaches, offset = log(exposure2))\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-17.9430   -5.1529   -3.8059    0.1452   26.7771  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  3.089e+00  2.123e-02  145.49   <2e-16 ***\nroach1       6.983e-03  8.874e-05   78.69   <2e-16 ***\ntreatment   -5.167e-01  2.474e-02  -20.89   <2e-16 ***\nsenior      -3.799e-01  3.342e-02  -11.37   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 16954  on 261  degrees of freedom\nResidual deviance: 11429  on 258  degrees of freedom\nAIC: 12192\n\nNumber of Fisher Scoring iterations: 6\n\n\nEl modelo completo da una impresión con buenos resultados, todas las variables son significativas pero los residuos no están centrados en cero, por ende no cumplen los supuestos iniciales. El siguiente código genera una muestra Bootstrap para los parámetros del modelo M_1.\n\n\nCode\nglm_boots = function(y,x,exposure,B = 1000){\n  n = length(y)\n  est = NULL\n  for (i in 1:B) {\n    si = sample(x = 1:n,size = n,replace = TRUE)\n    mli = glm(y[si]~x[si,], offset = log(exposure[si]),family = poisson)\n    ci = as.array(mli$coefficients)\n    est = rbind(est,ci)\n  }\n  # Estética\n  cn = colnames(x)\n  colnames(est) = c(\"intercepto\",cn)\n  \n  return(est)\n}\n\n\nObtenemos una muestra Bootstrap para los estimadores \\hat \\beta de tamaño B = 2,000 repeticiones\n\nbtp = glm_boots(y = roaches$y,\n               x = as.matrix(roaches[,2:4]),\n               exposure = roaches$exposure2,B = 2000)\n\ncolor_scheme_set(\"green\")\nmcmc_dens(btp)+labs(title=\"Distribución muestral de los estimadores, GLM Poisson\",\n                    subtitle =\"Bootstrap B = 2,000 iteraciones\")\n\n\n\n\nFigure 3: Gráfico de densidades. Cada densidad representa la distribución muestral aproximada para cada uno de los estimadores usando un Bootstrap de B = 2,000 iteraciones.\n\n\n\n\nLos intervalos de confianza al 95% son:\n\nx = apply(btp,MARGIN = 2, FUN = quantile, probs = c(0.025,0.5,0.975)) \n\n# Estética\nx = data.frame( t(x) )\nx$pars = c(\"intercepto\",\"roach1\",\"treatment\",\"senior\")\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"parámetros\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\nTable 1:  Intervalos de confianza al 95%, obtenidos a partir de una muestra bootstrap de B = 2,000 iteraciones parámetrosq2.5%Medianq97.5%intercepto2.676509273.0712688073.443233437roach10.005552250.0071557070.009586222treatment-1.00072833-0.546522828-0.102047046senior-1.07258930-0.3853403110.146554125\n\n\n\nLos intervalos de confianza muestran que el efecto de la variable roach1 esta concentrado en cero, por lo tanto, se deberá considerar un GLM de Poisson excluyendo dicha variable."
  },
  {
    "objectID": "ABM/posts/NB-GLM/index.html#ajuste-del-modelo-log-normal",
    "href": "ABM/posts/NB-GLM/index.html#ajuste-del-modelo-log-normal",
    "title": "Tratamiento para cucarachas",
    "section": "Ajuste del modelo log-normal",
    "text": "Ajuste del modelo log-normal\nAjustamos el modelo GLM log-normal completo que consiste en usar todas las variables, y revisamos el ajuste e inferencia de los parámetros. Hay que tomar en cuenta que al tomar y y roach1 en escala logarítmica, se tendrán que descartar los valores infinitos obtenidos, dado que las distribuciones se acumulan en cero\n\ndf = roaches\ndf[,1:2] = log(df[,1:2])\ndf = subset(df,subset = df$y != -Inf & df$roach1 != -Inf)\n\nm2 = lm(y~.,data = df)\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0679 -1.0726  0.2976  0.9884  2.8697 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.58442    0.40132   3.948 0.000122 ***\nroach1       0.52136    0.06408   8.137 1.62e-13 ***\ntreatment   -0.38609    0.21808  -1.770 0.078746 .  \nsenior      -0.38632    0.25848  -1.495 0.137187    \nexposure2   -0.19054    0.30949  -0.616 0.539082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.299 on 146 degrees of freedom\nMultiple R-squared:  0.3315,    Adjusted R-squared:  0.3132 \nF-statistic:  18.1 on 4 and 146 DF,  p-value: 4.281e-12\n\n\nEl modelo completo da una mala impresión,hay variables no significativas, el coeficiente de determinación \\hat R = 0.31 es bastante cercano a cero, y los residuos no están centrados en cero, por ende no cumplen los supuestos iniciales. Finalmente, revisamos los residuos del modelo, dado que los supuestos de normalidad pueden ser evaluados.\n\nautoplot(m2)\n\n\n\n\nFigure 4: Gráfico diagnóstico de los residuos, estos cuatro gráficos evaluan el ajuste y supuestos del modelo, si algún comportamiento polinómico es persistente, entonces los supuestos del modelo no se satisfacen.\n\n\n\n\nFigure 4 muestra que los supuestos de normalidad en su mayoría si se cumplen, el gráfico inferior izquierdo muestra un comportamiento irregular, pero debido a que una observación es anómala e influenciable, por lo tanto, otro modelo a considerar es usando una distribución Student-t con grados de libertad cercanos a v = 3."
  },
  {
    "objectID": "ABM/posts/NB-GLM/index.html#modelo-de-conteo-binomial-negativa",
    "href": "ABM/posts/NB-GLM/index.html#modelo-de-conteo-binomial-negativa",
    "title": "Tratamiento para cucarachas",
    "section": "Modelo de Conteo, Binomial Negativa",
    "text": "Modelo de Conteo, Binomial Negativa\nLa distribución Binomial negativa mide el número de éxitos que ocurren hasta el k-ésimo fracaso. Una v.a.d se distribuye Binomial Negativa (y_i \\sim BN(k,p)) si:\nf(y|p) = \\binom{y+k-1}{y}(1-p)^k p^y.\nDonde:\n\np es la probabilidad de éxito de un experimento Bernoulli.\nk es el número de fracasos hasta tener el primer éxito\nE[y_i] = \\frac{pk}{1-p} es el valor esperado.\nV[y_i] = \\frac{pk}{(1-p)^2}, es la varianza.\n\nEsta distribución se puede re-parametrizar en términos de su media E[y_i] = \\mu y varianza V[y] = \\sigma^2.\n\np = \\frac{\\sigma^2 - \\mu}{\\sigma^2},\nk = \\frac{\\mu^2}{\\sigma^2 - \\mu}.\n\nf(y) = \\binom{y+\\frac{\\mu^2}{\\sigma^2 - \\mu}-1}{y}\\left(\\frac{\\sigma^2 - \\mu}{\\sigma^2}\\right)^y \\left(\\frac{\\mu}{\\sigma^2}\\right)^{\\frac{\\mu^2}{\\sigma^2 - \\mu}}\nPara medir los efectos del tratamiento mediante un GLM de conteo de Binomial negativa, definimos la verosimilitud de tal forma que:\ny_i \\sim BN(\\mu_i,\\sigma^2)\\quad g(\\mu_i) = \\log(\\mu_i), \\text{ y } \\mu_i = \\mu_0e^{\\beta X_i}.\nDonde:\n\n\\mu_0 se le conoce como la información previo a la exposición.\ng:\\mathbb R \\to \\mathbb R, es la función de enlace logarítmica g(x) = \\log x\ny X son las covariables.\nEl modelo no tiene la limitante que la varianza es la media V[y_i] \\neq \\mu_i.\n\nAjustamos el modelo GLM de conteo completo que consiste en usar todas las variables, y revisamos el ajuste e inferencia de los parámetros.\n\nlibrary(MASS)\n\nm3 = glm.nb(y ~ roach1 + treatment + senior + exposure2,data = roaches)\nsummary(m3)\n\n\nCall:\nglm.nb(formula = y ~ roach1 + treatment + senior + exposure2, \n    data = roaches, init.theta = 0.2717608411, link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8052  -1.3452  -0.6815  -0.0344   3.1831  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.393790   0.441811   5.418 6.02e-08 ***\nroach1       0.012764   0.001601   7.975 1.52e-15 ***\ntreatment   -0.764550   0.245659  -3.112  0.00186 ** \nsenior      -0.341062   0.264555  -1.289  0.19733    \nexposure2    0.435159   0.374132   1.163  0.24478    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.2718) family taken to be 1)\n\n    Null deviance: 338.89  on 261  degrees of freedom\nResidual deviance: 277.87  on 257  degrees of freedom\nAIC: 1792.7\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2718 \n          Std. Err.:  0.0259 \n\n 2 x log-likelihood:  -1780.7380 \n\n\nEl modelo completo presenta mejores resultados que el modelo de Poisson pese que no todas las variables son significativas. Los residuos están centrados en cero y menos disperso, cumpliendo los supuestos iniciales."
  },
  {
    "objectID": "ABM/posts/NB-GLM/index.html#selección-de-modelos-5-fold-cv",
    "href": "ABM/posts/NB-GLM/index.html#selección-de-modelos-5-fold-cv",
    "title": "Tratamiento para cucarachas",
    "section": "Selección de modelos, 5-fold CV",
    "text": "Selección de modelos, 5-fold CV\nPara seleccionar el mejor modelo usaremos validación cruzada, 5-fold, esto implica que ajustaremos cinco veces cada modelo, evaluando la capacidad de aprendizaje usando AIC, RMSE y MAE. Los modelos que se consideraran son los siguientes:\n\nM_1: Modelo de Poisson completo\nM_2: Modelo log-normal completo.\nM_3: Modelo Binomial Negativa completo.\nM_{3.1}: Modelo BN reducido, sin la variable roach1.\n\nEl siguiente código presenta una función para realizar k-fold-CV para cualquier valor de k. En caso de querer añadir otros modelos o criterios, la función deberá ser modificada.\n\n\nCode\nkfold = function(df,k){\n  # Generar la particion\n  kfld = createFolds(df[,1],k = k)\n  mat = NULL\n  \n  for (i in 1:k) {\n    # separar los datos en conjuntos de prueba y entrenamiento\n    dfE= df[-kfld[[i]],]\n    dfP = df[kfld[[i]],]\n    # Ajustar los modelos\n    m1  = glm(y ~ roach1 + treatment + senior, offset = log(exposure2),\n            data = dfE, family = poisson)\n    m3  = glm.nb(y ~ roach1 + treatment + senior + exposure2,data = roaches)\n    m31 =  glm.nb(y ~ treatment + senior + exposure2,data = roaches)\n    \n    p1  = predict(m1,dfP)\n    p3  = predict(m3,dfP)\n    p31 = predict(m31,dfP)\n    \n    # Calcular MAE\n    mae = c(\n             MAE(pred =  p1,obs = dfP[,1]),\n             MAE(pred =  p3,obs = dfP[,1]),\n             MAE(pred =  p31,obs = dfP[,1])\n            )\n\n    # Unir los datos\n    mat = rbind(mat,mae) \n  }\n  colnames(mat) = c(\"MAE1\",\"MAE3\",\"MAE31\")\n  row.names(mat) = NULL\n  return(mat)\n}\n\nkfold1 = function(df,k){\n  # Generar la particion\n  kfld = createFolds(df[,1],k = k)\n  mat = NULL\n  \n  for (i in 1:k) {\n    # separar los datos en conjuntos de prueba y entrenamiento\n    dfE= df[-kfld[[i]],]\n    dfP = df[kfld[[i]],]\n    # Ajustar los modelos\n    m2 = lm(y ~ .,data = dfE)\n    p2  = predict(m2,dfP)\n\n    # Unir los datos\n    mat = rbind(mat,MAE(pred =  p2,obs = dfP[,1])) \n  }\n  colnames(mat) = c(\"MAE2\")\n  row.names(mat) = NULL\n  return(mat)\n}\n\n\nTable 2 compara los modelos de conteo, se observa que en la mayoría de criterios el modelo Binomial Negativa completo, presenta los mejores resultados y el mejor ajuste, por lo tanto, seleccionamos al modelo M_3 que este deberá ser comparado con el modelo log-normal.\n\n\nCode\nrst = kfold(df = roaches,k = 5)\nx = t(apply(rst,MARGIN = 2,FUN = \"quantile\",probs = c(0.025,0.5,0.975)))\n\n# Estética\nx = data.frame(x)\nx$pars =  c(\"M1\",\"M3\",\"M3.1\")\n\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"MAE\")\n\nrst1 = kfold1(df = df,k = 5)\nx1 = t(apply(rst1,MARGIN = 2,FUN = \"quantile\",probs = c(0.025,0.5,0.975)))\n\n# Estética\nx1 = data.frame(x1)\nx1$pars = c(\"M2\")\ncolnames(x1) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"MAE\")\n\nx = rbind(x,x1)\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\n\nTable 2:  Criterios de información del modelo log-N. Mediante Validación cruzada. La tabla presenta los criterios AIC, RMSE, y MAE bajo un 5-fold cv. MAEq2.5%Medianq97.5%M118.65001825.00602131.710160M318.51176324.92722931.506590M3.118.82821225.18210431.834678M21.0566121.0808531.204774\n\n\n\nContrario a los esperado, el modelo log-normal M_2 presenta mejores resultados que los modelos de conteos, por lo tanto el mejor modelo para medir el efecto de tratamientos en cucarachas desde un enfoque de aprendizaje es el modelo log-normal.\nEs importante resaltar que las predicciones realizadas con el modelo M_2 se realizaron en escala logarítmica,\nRMSE = \\frac{1}{\\sqrt n}||\\hat{\\log (y_P)} -\\log (y_P)||_2. Es necesario revisar si al transformar de forma inversa el modelo mantiene las predicciones. En caso de evaluar las predicciones en la escala natural de los datos, corroborar si:\nRMSE = \\frac{1}{\\sqrt n}||e^{\\mu_p+0.5\\sigma} -y_P||_2. Donde \\mu_P son las predicciones obtenidas del modelo en escala logarítmica."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html",
    "href": "ABM/posts/GLMs/index.html",
    "title": "Modelos lineales generalizados aplicados",
    "section": "",
    "text": "Este post resume el workflow básico para realizar un análisis estadístico en un enfoque clásico.\nEl modelo lineal generalizado (GLM) relaciona de forma funcional un conjunto de variables aleatorias Z = (Y,X), donde la v.a. Y \\in \\mathbb{R} se le conoce como variable dependiente, y a la v.a. X \\in \\mathbb{R}^d son las covariables.\nSea Z = \\{Z_1,Z_2,\\ldots,Z_n\\} un conjunto de variables aleatorias independientes, tal que Z_i = (Y_i,X_i) \\in \\mathbb R^{d+1}, decimos que Z sigue un GLM si:\nY_i \\sim \\mathscr{F}_\\varepsilon (\\theta_i), \\quad \\text{y } g(\\theta_i) = \\beta X_i. Donde:"
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#log-verosimilitud",
    "href": "ABM/posts/GLMs/index.html#log-verosimilitud",
    "title": "Modelos lineales generalizados aplicados",
    "section": "Log-Verosimilitud",
    "text": "Log-Verosimilitud\nLos GLMs son modelos probabilistas, cuya función de probabilidad se establece mediante la verosimilitud en escala logarítmica. Dado el supuesto de independencia de los datos, la verosimilitud es simplemente el producto de las densidades marginales de los datos.\nL(y;\\theta) = f(y_1,y_2,\\ldots,y_n | \\theta) = \\prod_{i=1}^nf(y_i\\theta).\nDado que los GLM pertenecen a la familia exponencial, la verosimilitud se puede expresar de forma analítica como:\nL(y;\\theta) = \\prod_{i=1}^n \\exp \\left[y_i b(\\theta_i)+c(\\theta_i)+d(y_i) \\right].\nFinalmente, la log-verosimilitud que es el logaritmo de L, también posee forma analítica\nl(y;\\theta) = \\sum_{i=1}^n y_i b(\\theta_i) +\\sum_{i=1}^nc(\\theta_i) + \\sum_{i=1}^n d(y_i)."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#estimación-de-los-parámetros",
    "href": "ABM/posts/GLMs/index.html#estimación-de-los-parámetros",
    "title": "Modelos lineales generalizados aplicados",
    "section": "Estimación de los parámetros",
    "text": "Estimación de los parámetros\nEl método de optimización más popular en GLMs es Máxima Verosimilitud que consiste en optimizar la log-verosimilitud, o simplemente resolver la función de score.\nU(y;\\theta) = \\frac{\\partial}{\\partial \\theta} l(y;\\theta) = 0. Generalmente resolver U implica resolver un sistema de ecuaciones no lineales, y al inicio el método más utilizado es el algoritmo de Newton. En la actualidad dicho algoritmo a evolucionado a un Limited Memory Broyden–Fletcher–Goldfarb–Shannon algorithm. El algoritmo L-BFGS es un método de Quasi-Newton que\n\nAproxima la matriz Jacobiana usando el método de Broyden, y\nAplica el algoritmo de Fletcher para corregir por estabilidad la aproximación de Broyden."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#incertidumbre-de-los-estimadores",
    "href": "ABM/posts/GLMs/index.html#incertidumbre-de-los-estimadores",
    "title": "Modelos lineales generalizados aplicados",
    "section": "Incertidumbre de los estimadores",
    "text": "Incertidumbre de los estimadores\nUn estimador es cualquier estadístico w(Y) que se utiliza para inferir información de \\beta. Dado que los estimadores \\hat \\beta = w(Y) son transformaciones de la muestra, poseen una distribución1.\nw(Y) \\sim f(w|\\beta).\nEn la mayoría de los casos, la distribución muestral no tiene forma analítica, y esta se aproxima usando remuestreo.\nLos algoritmos de remuestreo más utilizados son\n\nJackniffe\nBootstrap\n\nDe los dos algoritmos el más popular es Bootstrap, que consiste en generar una muestra de estimadores que aproxima la distribución deseada. El algoritmo es:\n\nElegir el número de sub-muestras B\nPara b =1,2,3,\\ldots, B hacer:\n\nExtraer una sub-muestra Y_b con reemplazo de Y,\nEstimar los parámetros del modelo \\hat \\beta_b con la sub-muestra Y_b.\n\nLa colección \\hat \\beta_1,\\hat \\beta_2, \\hat \\beta_3 ,\\ldots,\\hat \\beta_B es una muestra de la distribución muestral de los estimadores \\hat \\beta.\nUsar \\hat \\beta_1,\\hat \\beta_2, \\hat \\beta_3 ,\\ldots,\\hat \\beta_B para aproximar los intervalos de confianza de \\hat \\beta.\n\nEl método de Jackniffe es un caso particular del Bootstrap que consiste en extraer la sub-muestra Y_b eliminando la b-ésima observación y_b de la muestra original.\nPara análisis de incertidumbre de los estimadores, el Bootstrap provee mejores resultados que el método de Jackniffe, pero el segundo tiene su nicho al comparar diferentes modelos, validación cruzada."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#análisis-de-los-residuos.",
    "href": "ABM/posts/GLMs/index.html#análisis-de-los-residuos.",
    "title": "Modelos lineales generalizados aplicados",
    "section": "Análisis de los residuos.",
    "text": "Análisis de los residuos.\nLos residuos de un modelo se definen como la diferencia entre el valor ajustado por el modelo \\hat Y y su valor real Y.\nR_i = \\hat Y_i - Y_i.\nEn GLMs solo se revisan generalidades simples de los supuestos como:\n\nQue estén centrados en cero R_i \\approx 0.\nSean de varianza homogénea \\sigma_R\n\nEn modelos lineales simples, se pueden revisar más supuestos como normalidad, homogeneidad, y estacionaridad, ya que los residuos estiman los errores del modelo R_i = \\hat \\varepsilon_i.\nUn estadístico importante obtenido de los residuos, es el coeficiente de determinación R^2 que establece el porcentaje de varianza explicada por el modelo.\nR^2 = 1 - \\frac{\\sigma_R/(n-d-1)}{V[y]/(n-1)}"
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#selección-de-modelos",
    "href": "ABM/posts/GLMs/index.html#selección-de-modelos",
    "title": "Modelos lineales generalizados aplicados",
    "section": "Selección de modelos",
    "text": "Selección de modelos\nEn la práctica, es posible que se desarrollen múltiples modelos que expliquen el mismo conjunto de datos Z, existen indicadores que permiten describir las cualidades del modelo, los más utilizados son:\n\nlog-verosimilitud: \\log L= -l(y;\\theta).\nCriterio de Información de Bayes: BIC = 2 \\log L - 2\\log(n^d)\nCriterio de Información de Akaike AIC - 2 \\log L -2 d\nRoot Mean square error RMSE = \\frac{1}{\\sqrt n}||R||_2\nMean Absolute Error MAE = \\frac{1}{n}||R||_1\nMean Absolute Percentaje Error MAPE =\\frac{1}{n}\\sum_{i=1}^n \\left| \\frac{R_i}{Y_i} \\right|\n\n\nSelección de variables\nUna aplicación de la selección de modelos es encontrar modelos reducidos, esto es encontrar un subconjunto de variables X_1 \\subset X tal que el modelo reducido M_r sea lo mas parsimonioso posible. Un modelo parsimonioso brinda mayor explicabilidad, mayor capacidad de aprendizaje y mayor capacidad de generalización.\nLos métodos para reducción de variables se les conoce como búsquedas, y los tipos son:\n\nBúsquedas hacia adelante\nBúsquedas hacia atrás.\n\nEn búsqueda hacia adelante se inicia con el modelo más pequeño posible, se mide su criterio de información, y luego se agregan variables de forma secuencial de tal forma que el criterio mejore."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#aprendizaje",
    "href": "ABM/posts/GLMs/index.html#aprendizaje",
    "title": "Modelos lineales generalizados aplicados",
    "section": "Aprendizaje",
    "text": "Aprendizaje\nLos indicadores presentados miden el ajuste del modelo, esto es la capacidad del modelo de explicar el conjunto de datos. En aplicaciones mas recientes, la explicabilidad es una propiedad poco deseable, se prefiere medir la capacidad de aprendizaje.\n\n\n\n\n\n\nAprendizaje\n\n\n\nLa capacidad de aprendizaje es la habilidad del modelo de explicar propiedades externas a partir de la información adquirida.\n\n\nEn términos probabilistas, es la habilidad del modelo de predecir un nuevo conjunto de datos, a partir de los datos disponibles.\n\nParticiones\nEl aprendizaje se puede medir usando una partición de los datos. Sea Y = \\{Y_1,Y_2,\\ldots,Y_n\\} una muestra aleatoria, definimos una partición de entrenamiento y prueba como\nY = Y_E \\bigcup Y_P Donde:\n\nY_E \\bigcap Y_P = \\emptyset.\nm+k = n y m >> k.\nY_E = \\{Y_1,Y_2,\\ldots,Y_m\\} \\subset Y es el conjunto de entrenamiento.\nY_P = \\{Y_1,Y_2,\\ldots,Y_k\\} \\subset Y es el conjunto de prueba.\n\nEl algoritmo para medir aprendizaje es:\n\nAjustar los modelos M_1,M_2,\\ldots,M_f usando el conjunto de entrenamiento Y_E.\nPara cada modelo M_j hacer:\n\nRealizar k predicciones \\hat Y_{P,1},\\hat Y_{P,2},\\ldots,\\hat Y_{P,k}.\nComparar \\hat Y_{P} con Y_P usando AIC, BIC log-lik, RMSE o MAPE.\n\nEl modelo con mayor aprendizaje es el modelo con criterio menor.\n\nLa mayor limitante de las particiones es que al ser aleatorias los resultados varian bastante según la selección de las observaciones en el conjunto de entrenamiento. Una forma de evitar esos problemas es usando Validación cruzada."
  },
  {
    "objectID": "ABM/posts/GLMs/index.html#validación-cruzada",
    "href": "ABM/posts/GLMs/index.html#validación-cruzada",
    "title": "Modelos lineales generalizados aplicados",
    "section": "Validación cruzada",
    "text": "Validación cruzada\nValidación Cruzada consiste en realizar el proceso de partición muchas veces, el método mas utilizados es k-fold cross-validation este consisten en:\n\nDefinir m como el número de iteraciones a realizar\nPara i = 1,2,3,\\ldots, m hacer:\n\ndefinir el conjunto de prueba Y_p de tamaño k.\ndefinir el conjunto de entrenamiento Y_E como el complemento Y_E = Y_P^c.\nEstimar los modelos con Y_E\nComparar las predicciones de cada modelo con Y_P mediante algún criterio de información.\n\nPromediar los criterios obtenidos de cada iteración.\nElegir el modelo con criterio promedio menor.\n\nCuando el conjunto e prueba solo posee una observación, al método se le conoce como LOO (Leave one out cross validation). Otro método es utilizar un Bootstrap pero es altamente costoso.\n\nNotar que:\n\nLOO es equivalente a un muestreo de Jackniffe.\nCon validación cruzada re-utilizamos la información\nUsamos toda la muestra para validar los resultados.\nSe minimiza la variación de errores de aprendizaje."
  },
  {
    "objectID": "ABM/posts/Poisson/index.html",
    "href": "ABM/posts/Poisson/index.html",
    "title": "Efectividad de repelentes de insectos",
    "section": "",
    "text": "Este post muestra los pasos del Bayesian Workflow para analizar la base de datos Insects-Spray.\nSe desea evaluar la efectividad de seis diferentes tipos de repelentes para insectos (A,B,\\ldots,F). El experimento consiste en contar el número de insectos en cierta área delimitada, horas después de aplicar cierto tipo de repelente. El experimento se replicó 72 veces, 12 veces para cada uno de los tipos de repelente.\nLa Figure 1 resume la distribución y valores medios del número de insectos encontrados en cada uno de los tipos de repelente utilizados. Claramente, las distribuciones son no simétricas y muy heterogéneas entre cada uno de los tipos, y los datos son de tipo discreto, por lo tanto se descartan los supuestos de normalidad.\nSupondremos tres modelos distintos:\nM_1: \\ y_i \\sim Poisson(\\lambda), \\quad \\lambda \\sim Gamma(\\alpha,\\beta).\nM_2: \\ y_i \\sim \\text{Neg-Binom}(n,p), \\quad \\lambda \\sim U(1,1).\nM_3: \\ y_i \\sim Poisson(\\lambda), \\quad \\lambda \\sim N(\\mu,\\sigma^2).\nNote que en cada modelo, no consideramos la dispersión entre grupos presente en los datos, simplemente queremos modelar la distribución global del número de insectos contabilizados independiente del repelente utilizado.\nLos ejemplos propuestos se resolverán de tres diferentes formas:"
  },
  {
    "objectID": "ABM/posts/Poisson/index.html#modelo-m1-de-poisson-conjugado",
    "href": "ABM/posts/Poisson/index.html#modelo-m1-de-poisson-conjugado",
    "title": "Efectividad de repelentes de insectos",
    "section": "Modelo M1 de Poisson Conjugado",
    "text": "Modelo M1 de Poisson Conjugado\nSe define el número de insectos encontrado como una variable aleatoria discreta de tipo Poisson con parámetro \\lambda desconocido (y_i \\sim Poisson(\\lambda)) y aleatorio, la priori para \\lambda es una prior conjugada y “poco informativa”. Asumamos que los repelentes aplicados son efectivos y elegir una prior Gamma tal que centre la probabilidad en valores cercanos al cero.\n\nSelección de la prior\nEn este caso, la prior a usar es:\n\\lambda \\sim Gamma(3,3). Figure 2 presenta posibles candidatos a priors para el parámetro \\lambda.\n\nx = c(rgamma(600,2,4),rgamma(600,3,3),rgamma( 600,5,2))\ny = c(rep(\"G(2,4)\",600),rep(\"G(3,3)\",600),rep(\"G(5,2)\",600))\n\ndf = data.frame(sim = x, dist = y)\n\nggplot(aes(x = sim,fill = dist),data = df)+\n  geom_density(alpha = 0.4)+\n  labs(title = \"Diferentes tipos de priors\",\n       x = \"Simulaciones\",y = \"densidad\")\n\n\n\n\nFigure 2: Gráfico de densidades. Se comparan tres distribuciones Gamma como posibles candidatos para priors de lambda, las tres priors tienen medias similares alrededor de 9, pero la prior G(2,4) es la que provee mayor dispersión, siendo más sujetiva.\n\n\n\n\nLa distribución a posteriori para \\lambda es:\n\\lambda |y \\sim Gamma(3 + \\sum_{i=1}^n y_i,3+n).\nDonde, \\sum_{i=1}^n y_i = 684 y n = 72. Por lo tanto la posterior final es: \\lambda |y \\sim Gamma (687,75).\n\n\nEstimadores puntuales\nEl estimador puntual para \\lambda es:\n\\hat \\lambda = E[\\lambda | y] = \\frac{687}{75} \\approx 9.16. Figure 3 muestra la comparación de las densidades prior y posterior para el modelo Poisson, pese que la prior se eligió con el propósito de ser no informativa, la posterior es muy especifica y centrada en su primer momento.\n\nx = c(rgamma(600,3,3),rgamma(600,687,75))\ny = c(rep(\"Prior\",600),rep(\"Posterior\",600))\n\ndf = data.frame(sim = x, dist = y)\n\nggplot(aes(x = sim,fill = dist),data = df)+\n  geom_density(alpha = 0.4)+\n  labs(title = \"Comparación Prior | Posterior\",\n       subtitle = \"Modelo Poisson Conjugado\",\n       x = \"Simulaciones\",y = \"densidades\")\n\n\n\n\nFigure 3: Gráfico de densidades. Comparamos la distribuciones a Priori | Posteriori para el modelo Poisson conjugado, pese que la prior esta focalizada cerca de cero, la posterior se corre a la media de los datos, con muy poca dispersión, siendo muy especifica.\n\n\n\n\nLos intervalos de credibilidad se encuentran mediante Monte-Carlo. Los intervalos de credibilidad al 90% para la posterior son:\n\nIC = quantile(rgamma(600,687,75),probs = c(0.05,0.95))\nIC\n\n      5%      95% \n8.554635 9.798944 \n\n\n\n\nPosterior predictive checks\nEn este caso se realizaran comparaciones de la densidad predictiva del modelo, esta se puede estimar con Monte-Carlo mediante el siguiente procedimiento.\n\nPara k = 1,2,\\ldots,w; hacer\n\n1.1 Simular un valor \\lambda_k de la posterior Gamma(687,75).\n1.2 Simular un valor de la verosimilitud y^*_k \\sim Poisson(\\lambda_k).\n\nLos valores y^*_1,y^*_2,\\ldots,y^*_w son una muestra de y^* | y.\n\nFigure 4 compara las predictivas distribuciones obtenidas, notamos que la función predictiva esta muy centrada en el valor esperado de los datos, y provee un ajuste muy pobre.\n\nbayesplot_theme_set(theme_grey())\n# Predictive\nyrep = rep(rpois(600,rgamma(600,687,75)),72)\nyrep = matrix(yrep,ncol = 72,byrow = TRUE)\n\ny = InsectSprays$count\n\nppc_dens_overlay(y, yrep[1:200,])+\n  labs(title = \"Posterior Predictive Checks\",\n       subtitle = \"Modelo Poisson Conjugado\")\n\n\n\n\nFigure 4: Gráfico de densidades. Comparamos la distribuciones a predictiva y muestra para el modelo Poisson conjugado."
  },
  {
    "objectID": "ABM/posts/Poisson/index.html#modelo-m_2-de-binomial-negativa-con-priori-dispersa",
    "href": "ABM/posts/Poisson/index.html#modelo-m_2-de-binomial-negativa-con-priori-dispersa",
    "title": "Efectividad de repelentes de insectos",
    "section": "Modelo M_2 de Binomial Negativa con priori dispersa",
    "text": "Modelo M_2 de Binomial Negativa con priori dispersa\nSe define el número de insectos encontrado como una variable aleatoria discreta Binomial negativa con parámetro p desconocido y m = 30 repeticiones, (y_i \\sim \\text{Neg=Binom}(30,p)). La prior es conjugada para p es la distribución Beta.\np \\sim Beta(\\alpha,\\beta).\nEsta prior genera una posterior de tipo Beta, pero si \\alpha = \\beta = 1 la distribución es uniforme en el intervalo unitario, que a su vez es una prior dispersa para p.\nLa posterior para p es:\np | y \\sim Beta(1 + 30n,1+\\sum_{i=1}^n y_i).\nPor lo tanto, la posterior final es: p |y \\sim Beta (2160,685). Los estimadores puntuales y por intervalos son:\n\\hat p = E[p | y] = \\frac{2160}{2160+685} \\approx 0.759, y los intervalos de credibilidad al 90% :\n\nIC = quantile(rbeta(600,2160,685),probs = c(0.05,0.95))\nIC\n\n       5%       95% \n0.7465970 0.7713715 \n\n\nLos dos valores anteriores no brindan información del fenómeno de estudio, para una mejor interpretador, calculamos el número de insectos esperado a posterior, esto es, el valor esperado de la función de probabilidad que modela los datos, calculado con el estimador puntual obtenido\nE[y|p = \\hat p] = \\frac{r(1-\\hat p)}{\\hat p} \\approx 10. El número esperado de insectos con el modelo Binomial negativa es de 10 insectos, que es mayor al obtenido por el modelo M_1 cuyo valor esperado fue de 9.16 insectos.\n\n\nCode\n# Código preliminar al gráfico\nbayesplot_theme_set(theme_grey())\n\n# Compare distributions\nx = c(rbeta(600,1,1),rbeta(600,2160,685))\ny = c(rep(\"Prior\",600),rep(\"Posterior\",600))\n\ndf = data.frame(sim = x, dist = y)\n\n# posterior predictive checks\nyrep = rep(rnbinom(600,size = 30,rbeta(600,2160,685)),72)\nyrep = matrix(yrep,ncol = 72,byrow = TRUE)\n\ny = InsectSprays$count\n\n\n\nggplot(aes(x = sim,fill = dist),data = df)+\n  geom_density(alpha = 0.4)+\n  labs(title = \"Comparación Prior | Posterior\",\n       subtitle = \"Modelo Binomial-Negativa dispersa\",\n       x = \"Simulaciones\",y = \"densidades\")\n\nppc_dens_overlay(y, yrep[1:200,])+\n  labs(title = \"Posterior Predictive Checks\",\n       subtitle = \"Modelo Binomial Negativa dispersa\")\n\n\n\n\n\n\n\n(a) Comparamos la densidades a Priori | Posteriori y obtenemos una posterior muy especifica pese lo dispersa que es la prior.”\n\n\n\n\n\n\n\n(b) La densidad predictiva del modelo muestra el mal ajuste a los datos, el modelo no captura la dispersion y asimetría de la muestra.”\n\n\n\n\nFigure 5: Comparación de la posterior vs prior y análisis del ajuste del modelo mediante la densidad predictiva, modelo Binomial negativa.\n\n\n\nFigure 5 muestra la comparación de las densidades prior y posterior para el modelo Binomial negativa, pese la sobre-dispersión de la prior, se obtiene una posterior muy informativa y especifica. El gráfico derecho compara las predictivas obtenidas, notamos que la función predictiva esta muy centrada en el valor esperado de los datos, y provee un ajuste muy pobre."
  },
  {
    "objectID": "ABM/posts/Poisson/index.html#modelo-m_3-de-poisson-con-prior-débil",
    "href": "ABM/posts/Poisson/index.html#modelo-m_3-de-poisson-con-prior-débil",
    "title": "Efectividad de repelentes de insectos",
    "section": "Modelo M_3 de Poisson con prior débil",
    "text": "Modelo M_3 de Poisson con prior débil\nSe define el número de insectos encontrado como una variable aleatoria discreta de tipo Poisson con parámetro \\lambda desconocido (y_i \\sim Poisson(\\lambda)) y aleatorio, la prior para \\lambda es\n\\lambda \\sim N(8,1). En este caso la prior es no conjugada y se debe realizar el algoritmo de Metrópolis, Metropolis et al. (1953). El siguiente código muestra la implementación del algoritmo para nuestro modelo, la primera función calcula la densidad propuesta que es simplemente el producto de le verosimilitud y la prior, (f_p(\\theta) = f(y|\\theta)f(\\theta)). La segunda función es del algoritmo mismo.\n\n\nCode\npost = function(y,lambda,mu,sigma){\n  prior = dnorm(lambda,mean = mu,sd = sigma)\n  like = prod(dpois(x = y,lambda = lambda))\n  post = like*prior\n  \n  return(post)\n}\n\nmetropolis = function(y, mu, sigma = 1,iter = 5000,inits = rnorm(1, 5,1)){\n  lbd = rep(0,iter) \n  lbd[1] = inits\n  for (i in 2:iter) {\n    temp = rnorm(1,mean = lbd[i-1])\n    \n    p1 = post(y,lambda = temp,mu = mu,sigma = sigma)\n    p2 = post(y,lambda = lbd[i-1],mu = mu,sigma = sigma)\n    pa = p1/p2\n    \n    lbd[i] = lbd[i-1]\n    \n    if(pa > runif(1))\n      lbd[i] = temp\n      \n  }\n  return(lbd)\n}\n\n\nPara este ejemplo simulamos dos cadenas independientes de 5,000 iteraciones, donde el valor inicial de cada cadena se simuló de una normal con media cinco y varianza uno.\n\nlambda1 = metropolis(y = InsectSprays$count,mu = 8)\nlambda2 = metropolis(y = InsectSprays$count,mu = 8)\n\ndf1 = data.frame(chain = sort(rep(1:2,4000)),\n                lambda = c(lambda1[1001:5000],lambda2[1001:5000]))\npdf = posterior::as_draws(df1)\n\nmcmc_combo(x = df1,pars = \"lambda\")\n\n\n\n\nFigure 6: Gráfico de las posterior. Presentamos los traceplot y densidades de la posterior simulada, usando 2 cadenas de 5000 iteraciones y Warm-up: 1000 iteraciones.\n\n\n\n\nFigure 6 muestra los trace-plots de ambas cadenas que se entrelazan entre ellas indicando estacionariadad y convergenica, el grafico de densidades es uni modal y simétrico indicando convergencia. Table 1 muestra las estadísticas resumen de la posterior de \\lambda, dos indicadores importantes muestran convergencia de las cadenas, el effective sample size (ess) que indica el número de muestras independientes a las que equivalen las muestras obtenidas de la cadenas, dichos valores deben ser similar al número de iteraciones. El factor de convergencia \\hat R es un valor que compara las varianzas de las cadenas, valores aproximados a 1 indican convergencia.\n\nx = summarise_draws(pdf)\nft = flextable(x[2,])\nautofit(ft)\n\n\n\nTable 1:  Criterios de información de los modelos. Mediante Validación cruzada. La tabla presenta los criterios AIC, RMSE, MAE y MAPE bajo un 10-fold cv. variablemeanmediansdmadq5q95rhatess_bulkess_taillambda9.3416079.3326510.33591670.33186268.7982469.886041.0000661,555.8321,831.55\n\n\n\nTable 1 muestra la media a posterior e intervalos de credibilidad para la posterior de \\lambda. Además, se muestra el error de Monte-Carlo, dicho error debe ser cercano a 0, valores muy grandes indican alta dispersión de las simulaciones que se interpreta como una mala aproximación del método.\n\n\nCode\n# Preliminar\nbayesplot_theme_set(theme_grey())\n\nx = c(rnorm(600,8,1),pdf$lambda[1001:1600])\ny = c(rep(\"Prior\",600),rep(\"Posterior\",600))\n\ndf = data.frame(sim = x, dist = y)\n\n# Predictive\nyrep = rep(rpois(600,pdf$lambda[1001:1600]),72)\nyrep = matrix(yrep,ncol = 72,byrow = TRUE)\n\ny = InsectSprays$count\n\n\n\nggplot(aes(x = sim,fill = dist),data = df)+\n  geom_density(alpha = 0.4)+\n  labs(title = \"Comparación Prior | Posterior\",\n       subtitle = \"Modelo Poisson con prior normal\",\n       x = \"Simulaciones\",y = \"densidades\")\n\nppc_dens_overlay(y, yrep[1:200,])+\n  labs(title = \"Posterior Predictive Checks\",\n       subtitle = \"Modelo Poisson con Prior débil\")\n\n\n\n\n\n\n\n(a) Comparamos la densidades a Priori | Posteriori y obtenemos una posterior muy especifica pese lo dispersa que es la prior.”\n\n\n\n\n\n\n\n(b) La densidad predictiva del modelo muestra el mal ajuste a los datos, el modelo no captura la dispersion y asimetría de la muestra.”\n\n\n\n\nFigure 7: Comparación de la posterior vs prior y análisis del ajuste del modelo mediante la densidad predictiva, modelo de Poisson con prior no conjugada.\n\n\n\nFigure 7 muestra la comparación de las densidades prior y posterior para el modelo Poisson, la dinámica entre la prior y posterior es mas natural, pero la influencia de los datos hace que la prior sea de leve influencia en la posterior. El gráfico derecho compara las predictivas obtenidas, notamos que la función predictiva está muy centrada en el valor esperado de los datos, y provee un ajuste muy pobre.\n\nSelección de Modelos\nPara seleccionar el mejor modelo de los tres, utilizaremos los cuatro criterios definidos, Factor de Bayes, log-likelihood elpd, y WAIC.\n\nFactores de Bayes\nEl código para calcular la densidad marginal de cada modelo es:\n\n\nCode\nMarginal1 = function(y,iter = 10000){\n  mar = 1:iter\n  \n  for (i in 1:iter) {\n    mar[i] = sum(dpois(x = y,lambda = rgamma(n = 1,shape = 3,3),log = TRUE))\n  }\n  return(mean( exp(mar) ))\n}\nMarginal2 = function(y,iter = 10000){\n  mar = 1:iter\n  \n  for (i in 1:iter) {\n    mar[i] = sum(dnbinom(x = y,prob = rbeta(1,1,1),size = 30,log = TRUE))\n  }\n  return(mean( exp(mar) ))\n}\nMarginal3 = function(y,iter = 10000){\n  mar = 1:iter\n  \n  for (i in 1:iter) {\n    mar[i] = sum(dpois(x = y,lambda = rnorm(n = 1,mean = 8,sd = 1),log = TRUE))\n  }\n  return(mean( exp(mar) ))\n}\n\n\nLas estimaciones de Monte-Carlo, con m = 50,000 iteraciones para cada modelo son:\n\nm1 = Marginal1(y = InsectSprays$count,iter = 50000)\nm2 = Marginal2(y = InsectSprays$count,iter = 50000)\nm3 = Marginal1(y = InsectSprays$count,iter = 50000)\n\nLos factores de Bayes para comparar los 3 modelos son:\n\nFB = log(c(m1/m2,m1/m3,m2/m3))\nnames(FB) =c(\"log FB12\",\"log FB13\",\"log FB23\") \nFB\n\n  log FB12   log FB13   log FB23 \n-156.14747  -34.27028  121.87719 \n\n\nLas estimaciones obtenidas muestran una evidencia rotunda a preferir el modelo M_2 sobre el modelo M_1 y una evidencia fuerte de preferir el modelo M_1 sobre el modelo M_3. Finalmente, hay evidencia rotunda a predecir el modelo M_2 sobre el modelo M_3.\nPor lo tanto, el modelo selecionado es: M_2: Binomial negativa, con prior dispersa.\nEl mayor problema de los factores de Bayes son:\n\nLos modelos son inestables\nEl modelo es muy sensible a modelos priors no informativas o muy dispersas.\n\n\n\nlog-Verosimilitud\nUn estimador muy importante para la selección de modelos es la matriz de log-verosimilitudes, esta se estima por métodos de Monte-Carlo usando una muestra de la posterior \\theta_1,\\theta_2,\\ldots,\\theta_S, de la siguiente forma\n\\log f(y|\\theta) = [\\log f(y_i|\\theta_j)] \\in \\mathbb R^{S \\times n} Las siguientes lineas de código generan la matriz de verosimilitudes\n\n\nCode\nloglik1 = function(y,iter = 10000){\n  loglik  = matrix(nrow = iter, ncol = length(y))\n  \n  for (i in 1:iter)  \n    loglik[i, ] = dpois(y,lambda = rgamma(1,687,75),log = TRUE)\n  \n  return(loglik)\n}\n\nloglik2 = function(y,iter = 10000){\n  loglik  = matrix(nrow = iter, ncol = length(y))\n  \n  for (i in 1:iter)  \n    loglik[i, ] = dnbinom(y,size = 30,prob = rbeta(1,2160,685),log = TRUE)\n  \n  return(loglik)\n}\n\nloglik3 = function(y,lbd){\n  loglik  = matrix(nrow = length(lbd), ncol = length(y))\n  \n  for (i in 1:length(lbd))  \n    loglik[i, ] = dpois(y,lambda = lbd[i],log = TRUE)\n  \n  return(loglik)\n}\n\n\nA partir de las matrices de log-verosimilitudes se puede estimar una muestra a posteriori de la log-verosimilitud del modelo a partir de la siguiente ecuación\n\\log f(y| \\theta) = -\\sum_{i=1}^n \\log f(y_i | \\theta). Estos valores pueden utilizarse para comparación preliminar de modelos, pero como una medida absoluta, la siguiente figura muestra las posteriors de las log verosimilitudes:\n\nll1 = loglik1(y = InsectSprays$count,iter = 8000)\nll2 = loglik2(y = InsectSprays$count,iter = 8000)\nll3 = loglik3(y = InsectSprays$count,lbd = df1$lambda)\n\nlogVero = data.frame(\n      loglik = c(apply(-ll1,1,sum),apply(-ll2,1,sum),apply(-ll3,1,sum)),\n      models = c(rep(\"M1\",8000),rep(\"M2\",8000),rep(\"M3\",8000))\n    )\n\nggplot(aes(x = loglik,fill = models),data = logVero)+\n  geom_density(alpha = 0.4)+\n  labs(title = \"Comparación de Modelos\",\n       subtitle = \"Log-verosimilitudes\",\n       x = \"Simulaciones\",y = \"densidades\")\n\n\n\n\nLas log-verosimilitudes indican que el modelo M_22 con verosimilitud Binomial Negativa estima mucho mejor que los dos modelos con verosimilitud de Poisson.\n\n\nExpected log-Predictive density (elpd)\nLa elpd es una medida de divergencia entre el modelo ajustado y la distribución real de los datos que se calcula mediante la siguiente ecuación\nelpd(M_k|y) = - \\int\\log f(y^*|y) f_t(y)dy Esta propuesta esta implementada en el paquete loo, y se puede obtener a partir de la matriz log-verosimilitudes.\n\nlibrary(loo)\n\nloo1 = loo(ll1)\nloo2 = loo(ll2)\nloo3 = loo(ll3)\n\ncompare(loo1,loo2,loo3)\n\n     elpd_diff se_diff elpd_loo p_loo  looic \nloo2    0.0       0.0  -304.1      4.1  608.2\nloo3  -36.5       5.8  -340.6      4.8  681.2\nloo1  -37.2       6.5  -341.3      5.3  682.6\n\n\nEn este criterio el modelo M_2 representa el mejor modelo de los datos.\n\n\nCriterios de información de Watanabe\nEl criterio de información de Watanabe es asintomático al valor obtenido por la elpd, por lo tanto, puede ser aproximado con validación cruzada.\n\nwaic1 = waic(ll1)\nwaic2 = waic(ll2)\nwaic3 = waic(ll3)\n\ncompare(waic1,waic2,waic3)\n\n      elpd_diff se_diff elpd_waic p_waic waic  \nwaic2    0.0       0.0  -304.1       4.1  608.2\nwaic3  -36.5       5.8  -340.6       4.8  681.2\nwaic1  -37.2       6.5  -341.3       5.3  682.6\n\n\nFinalmente, elegimos al modelo M_2 como el mejor modelo que explica el número de insectos al aplicar un pesticida."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html",
    "href": "ABM/posts/Gaussian-LM/index.html",
    "title": "GLMs Gaussianos: Capacidad de motores de carros",
    "section": "",
    "text": "Este post presenta un análisis la base mtcars usando un modelo lineal múltiple con verosimilitud Gaussiana.\nLa base de datos mtcars contiene el registro de motores de carros mas populares en USA, 1974. los datos contienen 32 registros, con 10 atributos del motor.\nSe desea predecir la capacidad de consumo de los motores, para eso se evaluaron las siguiente variables."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#verosimilitud",
    "href": "ABM/posts/Gaussian-LM/index.html#verosimilitud",
    "title": "GLMs Gaussianos: Capacidad de motores de carros",
    "section": "Verosimilitud",
    "text": "Verosimilitud\nPara medir la relación de consumo de los motores utilizaremos un GLM normal tal que:\nmpg_i \\sim N(\\mu_i,\\sigma^2), \\quad  g(\\mu_i) = \\mu_i, \\text{ y } \\mu_i = \\beta X_i.\nEl siguiente código limpia la base de datos para obtener las variables de interés\n\ndf = mtcars[,c(1,4,6,8,10,11)]\nstr(df)\n\n'data.frame':   32 obs. of  6 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nTodas las variables son numéricas, pero algunas son totalmente enteras, dificultando el proceso de análisis, se procede a revisar las correlaciones para revisar las interacciones lineales entre variables.\n\nggpairs(df)\n\n\n\n\nFigure 1: Gráfico de pares. La diagonal principal muestra histogramas densidades de cada una de las variables. La parte superior muestra el coeficiente de correlación entre dos variables, fila y columna. La parte inferior muestra un gráfico de dispersión entre dos variables.\n\n\n\n\nFigure 1 muestra colinealidad entre las variables mpg, hp y wt. Por lo tanto, múltiples modelos deben ser considerados. Realizemos un modelo inicial, el considerado el modelo completo que posee todas las variables\n\n\n\n\n\n\nColinealidad\n\n\n\nDos covariables X_1 y X_2 se dicen ser colineales si las variables son linealmente dependientes.\nRecordar que si dos columnas de una matriz son linealmente dependiente, entonces el determinante es cero."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#ajuste-del-modelo",
    "href": "ABM/posts/Gaussian-LM/index.html#ajuste-del-modelo",
    "title": "GLMs Gaussianos: Capacidad de motores de carros",
    "section": "Ajuste del modelo",
    "text": "Ajuste del modelo\nAjustamos el modelo completo que consiste en usar todas las variables, y revisamos el ajuste e inferencia de los parámetros.\n\nm1 = lm(mpg~.,data = df)\nsummary(m1)\n\n\nCall:\nlm(formula = mpg ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2884 -1.4370 -0.3155  1.1697  5.8246 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 27.20311    5.74212   4.737 6.72e-05 ***\nhp          -0.02339    0.01353  -1.728   0.0958 .  \nwt          -2.74663    0.92005  -2.985   0.0061 ** \nvs           0.94692    1.36929   0.692   0.4954    \ngear         1.78520    1.12762   1.583   0.1255    \ncarb        -0.65498    0.57767  -1.134   0.2672    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.569 on 26 degrees of freedom\nMultiple R-squared:  0.8477,    Adjusted R-squared:  0.8184 \nF-statistic: 28.94 on 5 and 26 DF,  p-value: 7.653e-10\n\n\nDebido a la alta colinealidad entre las variables, pocos parámetros estimados son significativos. Procedemos a eliminar algunas variables del modelo. Eliminamos la variable wt al ser colineal con múltiples variables. Por lo tanto, el modelo inicial M_1 es:\n\nm1  =  lm(mpg~vs+hp+gear+carb,data = df)\nsummary(m1)\n\n\nCall:\nlm(formula = mpg ~ vs + hp + gear + carb, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8047 -2.3487 -0.0967  1.9188  6.7859 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.03756    3.67694   3.546  0.00145 ** \nvs           0.84671    1.55657   0.544  0.59093    \nhp          -0.03449    0.01480  -2.331  0.02747 *  \ngear         4.20129    0.89285   4.705 6.72e-05 ***\ncarb        -1.33338    0.60391  -2.208  0.03593 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.921 on 27 degrees of freedom\nMultiple R-squared:  0.7955,    Adjusted R-squared:  0.7652 \nF-statistic: 26.25 on 4 and 27 DF,  p-value: 5.825e-09"
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#incertidumbre-de-los-estimadores.",
    "href": "ABM/posts/Gaussian-LM/index.html#incertidumbre-de-los-estimadores.",
    "title": "GLMs Gaussianos: Capacidad de motores de carros",
    "section": "Incertidumbre de los estimadores.",
    "text": "Incertidumbre de los estimadores.\nPese que la función lm de R realiza un análisis de incertidumbre al presentar una Prueba-t de significacia para cada parámetro \\beta_i, no presenta los intervalos de confianza. Estos serán estimados con Bootstrap. La siguiente función obtiene una muestra Bootstrap de los parámetros desconocidos \\beta.\n\n\nCode\nlm_boots = function(y,x,B = 1000){\n  n = length(y)\n  est = NULL\n  for (i in 1:B) {\n    si = sample(x = 1:n,size = n,replace = TRUE)\n    mli = lm(y[si]~x[si,] )\n    ci = as.array(mli$coefficients)\n    est = rbind(est,ci)\n  }\n  # Estética\n  cn = colnames(x)\n  colnames(est) = c(\"intercepto\",cn)\n  \n  return(est)\n}\n\n\nObtenemos una muestra Bootstrap para los estimadores \\hat \\beta de tamaño B=5,000 repeticiones\n\nbtp = lm_boots(y = df$mpg,x = as.matrix(df[,-1]),B = 5000)\n\nbayesplot_theme_set(theme_grey())\nmcmc_dens(btp)+labs(title=\"Distribución muestral de los estimadores\",\n                    subtitle =\"Bootstrap B = 5,000 iteraciones\")\n\n\n\n\nFigure 2: Gráfico de densidades. Cada densidad representa la distribución muestral aproximada para cada uno de los estimadores usando un Bootstrap de B=5,000 iteraciones.\n\n\n\n\nLos intervalos de confianza al 95% son:\n\nx = apply(btp,MARGIN = 2, FUN = quantile, probs = c(0.025,0.5,0.975)) \n\n# Estética\nx = data.frame( t(x) )\nx$pars = c(\"intercepto\",\"hp\",\"wt\",\"vs\",\"gear\",\"carb\")\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"parámetros\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\nTable 1:  Intervalos de confianza al 95%, obtenidos a partir de una muestra bootstrap de B = 5,000 iteraciones parámetrosq2.5%Medianq97.5%intercepto12.2010013726.9783548137.948075491hp-0.04826082-0.023916720.002385223wt-4.74193174-2.63010138-0.965883340vs-2.045293740.721916473.043954024gear0.166021481.834458615.144794762carb-2.26408869-0.717631530.291145300\n\n\n\nLos intervalos de confianza revelan mayor información a lo obtenido por la prueba-t, parámetros como hp, y carb que son significativos en la prueba, no lo son mediante los intervalos. Esto indica la posibilidad de un modelo mucho mas reducido."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#análisis-de-los-residuos",
    "href": "ABM/posts/Gaussian-LM/index.html#análisis-de-los-residuos",
    "title": "GLMs Gaussianos: Capacidad de motores de carros",
    "section": "Análisis de los residuos",
    "text": "Análisis de los residuos\nUna vez evaluadas las estimaciones del modelo, es necesario revisar los residuos del mismo para corroborar supuestos, la siguiente linea de código presenta un resumen descriptivo de los residuos del modelo inicial M_1, en su mayoría parecen estar centrados en cero.\n\nsummary(m1$residuals)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-3.80470 -2.34875 -0.09674  0.00000  1.91880  6.78592 \n\n\nFigure 3 presenta una visualización típica para el diagnostico de los residuos, ninguna figura debe presentar un comportamiento polinómico a excepción del gráfico de quantiles (derecha superior), que debe seguir el comportamiento de una función lineal creciente.\n\nautoplot(m1)\n\n\n\n\nFigure 3: Gráfico diagnóstico de los residuos, estos cuatro gráficos evaluan el ajuste y supuestos del modelo, si algún comportamiento polinómico es persistente, entonces los supuestos del modelo no se satisfacen."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#selección-de-modelos",
    "href": "ABM/posts/Gaussian-LM/index.html#selección-de-modelos",
    "title": "GLMs Gaussianos: Capacidad de motores de carros",
    "section": "Selección de modelos",
    "text": "Selección de modelos\nAdicional al modelo M_1, ajustamos dos modelos mas:\nM_2: \\quad mpg \\sim N(hp+gear+carb,\\sigma^2), M_3: \\quad mpg \\sim N(wt+gear+carb,\\sigma^2).\n\n\nCode\nm2 = lm(mpg~hp+gear+carb,data = df)\nm3 = lm(mpg~wt+gear+carb,data = df)\n\n\nEl siguiente código calcula el RMSE de un modelo linea en el conjunto de entrenamiento.\n\n\nCode\nrmse = function(m){\n  mse = sum(m$residuals^2)/length(m$residuals)\n  return(sqrt(mse))\n}\n\n\n\nx = matrix(0,nrow = 4,ncol = 3)\nx[1,] = c(logLik(m1),logLik(m2),logLik(m3))\nx[2,] = c(AIC(m1),AIC(m2),AIC(m3))\nx[3,] = c(BIC(m1),BIC(m2),BIC(m3))\nx[4,] = c(rmse(m1),rmse(m2),rmse(m3))\n\n# Estética\nx = data.frame(x)\nx$pars =  c(\"logLik\",\"AIC\",\"BIC\",\"RMSE\")\ncolnames(x)  = c(\"Modelo 1\",\"Modelo 2\",\"Modelo 3\",\"Criterio\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\nTable 2:  Criterios de información de los modelos. Se selecciona el modelo con menores criterios. CriterioModelo 1Modelo 2Modelo 3logLik-76.986353-77.160742-75.196419AIC165.972706164.321484160.392838BIC174.767122171.650163167.721517RMSE2.6828642.6975252.536917\n\n\n\nTable 2 muestra la tabla de criterios de información para el conjunto de datos mtcars para sorpresa del lector el mejor modelo es el alternativo M_3 que usa la variable colineal wt en vez de hp."
  },
  {
    "objectID": "ABM/posts/Gaussian-LM/index.html#validación-cruzada",
    "href": "ABM/posts/Gaussian-LM/index.html#validación-cruzada",
    "title": "GLMs Gaussianos: Capacidad de motores de carros",
    "section": "Validación cruzada",
    "text": "Validación cruzada\nEl siguiente código presenta una función para realizar k-fold-CV para cualquier valor de k. En caso de querer añadir otros modelos o criterios, la función deberá ser modificada.\n\n\nCode\nkfold = function(df,k){\n  # Generar la particion\n  kfld = createFolds(df[,1],k = k)\n  mat = NULL\n  \n  for (i in 1:k) {\n    # separar los datos en conjuntos de prueba y entrenamiento\n    dfE= df[-kfld[[i]],]\n    dfP = df[kfld[[i]],]\n    # Ajustar los modelos\n    m1 = lm(mpg~vs+hp+gear+carb,data = dfE)\n    m2 = lm(mpg~hp+gear+carb,data = dfE)\n    m3 = lm(mpg~wt+gear+carb,data = dfE)\n    \n    p1 = predict(m1,dfP)\n    p2 = predict(m2,dfP)\n    p3 = predict(m3,dfP)\n    \n    # Calcular  RMSE\n    rmse = c(\n             RMSE(pred =  p1,obs = dfP[,1]),\n             RMSE(pred =  p2,obs = dfP[,1]),\n             RMSE(pred =  p3,obs = dfP[,1])\n             )\n    # Unir los datos\n    mat = rbind(mat,rmse) \n  }\n  colnames(mat) = c(\"RMSE1\",\"RMSE2\",\"RMSE3\")\n  row.names(mat) = NULL\n  return(mat)\n}\n\n\nTable 3 presenta los resultados obtenidos al realizar 5-fold-cv usando el rmse como criterio de información, el modelo M_3 presenta las mejores predicciones. Por lo tanto, M_3 es el modelo con Mayor aprendizaje.\n\nrst = kfold(df = df,k = 5)\nx = t(apply(rst,MARGIN = 2,FUN = \"quantile\",probs = c(0.025,0.5,0.975)))\n\n# Estética\nx = data.frame(x)\nx$pars =  c(\"M1\",\"M2\",\"M3\")\ncolnames(x) = c(\"q2.5%\",\"Median\",\"q97.5%\",\"RMSE\")\n\nft = flextable(x[c(4,1,2,3)])\nautofit(ft)\n\n\n\nTable 3:  Criterios de información de los modelos. Mediante Validación cruzada. La tabla presenta los RMSE de cada modelo. bajo un 5-fold cv. RMSEq2.5%Medianq97.5%M12.6482572.9244453.647122M22.6864542.8753133.667834M32.4163322.9167643.234804"
  },
  {
    "objectID": "ABM/index.html",
    "href": "ABM/index.html",
    "title": "Applied Bayesian Modeling",
    "section": "",
    "text": "Bayesian Poisson models\n\n\n\n\nBayesian\n\n\nanalysis\n\n\ncode\n\n\nPoisson\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nanalysis\n\n\ncode\n\n\nprior\n\n\nposterior\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\n  \n\n\n\n\n\nGAMs logísticos\n\n\n\n\nBinomial\n\n\nGAM\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\n  \n\n\n\n\n\nGLMs logísticos\n\n\n\n\nBinomial\n\n\nGLM\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\n  \n\n\n\n\n\nGLMs de conteo\n\n\n\n\nPoisson\n\n\nGLM\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nGaussian\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRegression\n\n\ncode\n\n\nGLMs\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nAsael Alonzo Matamoros.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\nintroduction\n\n\nBaseline\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "DOF/posts/WDF1/index.html",
    "href": "DOF/posts/WDF1/index.html",
    "title": "So WDF is going on with my life now?",
    "section": "",
    "text": "A story of the my last month.\nHello everyone! My name is Asael Alonzo Matamoros, and I just came back home (Honduras). After 2 years of living in Finland, I got depression, unfinished business, and a will to write about statistics (and obviously about the stupid Colombian telenovela I call my life).\nIt has been a month since I returned. The adaptation has been a shit show, but everything is not lost. Some things turn out to be good or even enjoyable. For example, I am attending three weddings, all of whom are from the same group of friends. Yes! I am the “single one, and out of this minor detail, I am happy to celebrate these significant events with them. And obviously, I can’t forget that now I live with five incredible cutenesses that are always happy to see me. Please check them out in the following picture.\nSo, now I want to answer the critical question: What the fuck is going on with my life?! Well, I don’t know, but who really does? A good answer might be that I am recovering from some exhausting experiences that came out this year. Out of the drama, I want to share three exciting projects with you (Yes, you! The person who might be reading this, and I gotta say I love you!).\nI can’t talk about my research now, but let’s talk about Beer; in the end, it is more fun."
  },
  {
    "objectID": "DOF/posts/WDF1/index.html#beer-and-science",
    "href": "DOF/posts/WDF1/index.html#beer-and-science",
    "title": "So WDF is going on with my life now?",
    "section": "Beer and Science",
    "text": "Beer and Science\nOne of my guilty pleasures is watching way too many YouTube videos. I love those informative (nerdy) vlogs in which people talk about their research, technology, or knowledge field. Let’s be honest; it is fun to watch thirteen-minute videos explaining how Einstein came up with the relativity theory idea. And if not, I like it anyway, and IDGAF about your opinion!\nThe project’s central idea is to share some content about what we are doing (We try to do research, but we suck at it) and motivate others in our work or at least to read a book. Not to forget that I am a materialistic fuck, and I love to hear public praise like a stupid diva (a considerable motivation for myself). So, after a minimal amount of consideration, I contacted my friend Monchys who has vast experience drinking (I mean researching) and discussed the idea with him. Look at us now! We got some shit done, and eureka! “Beer and science” was born!\n\nOur first event was “La radiación de Curie,” where we wanted to talk about radiation, its impact on society, radium, and the consequences of misusing technology. Juan gave a fantastic talk about the mechanism of radiation, illustrating the work of some extraordinary scientists in the field. Monchys gave some examples where radiation is applied in real-life applications. And I talked about the misuse of radium and its consequences for society. I connected the story with Machine Learning and discussed some cases where artificial intelligence has been misused. Yes! You got me in a box! I love to shit talk about ML.\n\nThe event was a relative success (let’s agree that 22 persons attending the event were a success), and we decided to make a second round. I hope we can agree on the following topic (spoilers alert, the next topic might be “Intelligence”). You are all invited, and I hope to see you around so we can have a funny nerd talk about science (and, of course, get drunk together).\nSeriously! If you are interested in our event, please subscribe for free here."
  },
  {
    "objectID": "DOF/index.html",
    "href": "DOF/index.html",
    "title": "Depression and other stories",
    "section": "",
    "text": "news\n\n\nintroduction\n\n\nBaseline\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Asael Alonzo Matamoros",
    "section": "",
    "text": "Hello, I’m Asael, a statistician who programs in R, loves-hates Python, plays too much Pokemon, talks a lot, is crazy about traveling, and gets depressed often. Welcome to my blog! :). If you want to know more about my work, please check this section where I brag about myself or take home a paper (CV) for free XD.\nImportant: If you are a developer, R fanatic, or statistician, please star my bayesforecast package."
  },
  {
    "objectID": "index.html#asael-an-attempt-at-websiteblogging-again",
    "href": "index.html#asael-an-attempt-at-websiteblogging-again",
    "title": "Asael Alonzo Matamoros",
    "section": "Asael, an attempt at website/Blogging, again?!",
    "text": "Asael, an attempt at website/Blogging, again?!\nYes! If you are reading me now, you might notice that I need practice in my writing, so I am doing this website for three different purposes:\n\nImprove my R, Python, and Julia skills by presenting some time-series analyses using those languages.\nPractice my English writing by saturating everyone, including myself.\nPresent all those little works and collaborations I do in a compact (bounded and closed) set."
  },
  {
    "objectID": "index.html#so-whats-going-on-with-my-life-now",
    "href": "index.html#so-whats-going-on-with-my-life-now",
    "title": "Asael Alonzo Matamoros",
    "section": "So, what’s going on with my life now?",
    "text": "So, what’s going on with my life now?\nAfter three long years abroad studying, traveling, researching, snow, journal rejections, and surviving this sh@#t-show (COVID). I decided to return home, which was a difficult and messy choice. This webpage is my new attempt to organize my thoughts after this unique experience. No more elevator talks! Let me introduce you to my blog: Depression and other stories!\nSpoilers alert: I like to sing out loud with my headphones on in public spaces, so if you find a dude badly singing at a coffee shop in front of the computer, it’s me! Please say hi! :)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I am a Bayesian Data scientist with a strong background in Bayesian theory, probabilistic programming, time-series analysis, actuarial science, risk modeling, mathematical decision-making, and package developing in R.\nKeywords: Forecasting, Bayesian data analyst, Time-series, Probabilistic ML, Mathematics.\n\n\n\nProgramming: R, Python, Julia, Octave, Matlab, C++, SQL, Git.\nStatistical tools: SPSS, STATA, R, Excel, Stan, PyMC, Tableau, PowerBI.\nLanguagues: Spanish (native), English (professional), Dutch (Beginner)."
  },
  {
    "objectID": "about.html#research-projects",
    "href": "about.html#research-projects",
    "title": "About me",
    "section": "Research projects",
    "text": "Research projects\n\nBayesian order identification of ARMA models with projection predictive inference. McLatchie Y., Alonzo Matamoros A., Vehtari A. (2022). In submission to AISTATS 2023.\nUncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison. Sivula T., Magnusson M., Alonzo Matamoros A., Vehtari A. (2022). Submitted to JMLR.\nThe Multiple Degrees of freedom Gaussian Process. Alonzo Matamoros A. (2021). Presented at the Workshop of Bayesian Stochastic Processes.\nBayesian time-series modeling with bayesforecast. Alonzo Matamoros A., Vehtari A. (2021). Presented at the International Society of Bayesian Analysis 2021.\nAn R package for Normality in Stationary Processes. Alonzo Matamoros A, Nieto-Reyes A. (2020). Submitted to Journal of R."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\n\nPh.D. candidate in Computer Science.\nAalto University, Finland | October 2020.\n\n\nM.Sc. in Mathematics and Computer Science.\nUniversidad de Cantabria, Spain | October 2019 - October 2020.\n\n\nCertification in Probability.\nSociety of Actuaries, U.S.A. | November 2016.\n\n\nB. Sc. in Mathematics and Computer Science.\nUniversidad Nacional Autonoma de Honduras | October 2010 - October 2015."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About me",
    "section": "Experience",
    "text": "Experience\n\nResearch contract\nFinnish Center of Artificial Intelligence | October 2020.\nStudied the uncertainty of Cross-Validation methods when comparing probabilistic machine learning models. Proposed an algorithm for automatic forecasting with probabilistic time-series models.\nSkills: R, Python, ML, Model Comparison, Data Visualization..\n\n\nAssistant Professor, Statistics Department\nUniversidad Nacional Autonoma de Honduras (UNAH) | October 2017 – October 2020.\nImparted Statistical Inference, Stochastic Processes, and Linear Models classes for the department of Statistics.\nSkills: Julia, STATA, Mathematics, Bayesian Statistics.\n\n\nData analyst\nInstituto Hondureño de Transporte Público | January 2017 – April 2018.\nDesigned statistical models for estimating the public transport demands using simulation and stochastic processes. Adapted dynamic econometric models to analyze Tegucigalpa, Honduras’s public transport system.\nSkills: Simulation, Probability, Mathematical Modeling, Bayesian Statistics.\n\n\nActuarial analyst\nInstituto Hondureño de Seguridad Social | January 2016 – January 2017.\nWrote scripts for data cleaning and summaries for the actuarial evaluation of the total of affiliates in Honduras’ primary pension system. Created big-data modules for the actuarial analysis of the pension system.\nSkills:SQL, Data-cleaning, Big-data, Actuarial Modeling.\n\n\nStatistical Consultant\nInstituto Hondureño de Turismo | January 2015 – January 2016.\nDesigned the Sampling procedure for estimating the Touristic Expenditure in Honduras. I implemented time-series models for analyzing the principal financial indicators related to tourism.\nSkills: SPSS, Excell, forecasting, time-series."
  }
]