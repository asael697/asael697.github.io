[
  {
    "objectID": "posts/GLMs/index.html",
    "href": "posts/GLMs/index.html",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "",
    "text": "Este post resume el workflow básico para realizar un análisis estadístico en un enfoque clásico.\nEl modelo lineal generalizado (GLM) relaciona de forma funcional un conjunto de variables aleatorias Z = (Y,X), donde la v.a. Y \\in \\mathbb{R} se le conoce como variable dependiente, y a la v.a. X \\in \\mathbb{R}^d son las covariables.\nSea Z = \\{Z_1,Z_2,\\ldots,Z_n\\} un conjunto de variables aleatorias independientes, tal que Z_i = (Y_i,X_i) \\in \\mathbb R^{d+1}, decimos que Z sigue un GLM si:\nY_i \\sim \\mathscr{F}_\\varepsilon (\\theta_i), \\quad \\text{y } g(\\theta_i) = \\beta X_i. Donde:"
  },
  {
    "objectID": "posts/GLMs/index.html#log-verosimilitud",
    "href": "posts/GLMs/index.html#log-verosimilitud",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Log-Verosimilitud",
    "text": "Log-Verosimilitud\nLos GLMs son modelos probabilistas, cuya función de probabilidad se establece mediante la verosimilitud en escala logarítmica. Dado el supuesto de independencia de los datos, la verosimilitud es simplemente el producto de las densidades marginales de los datos.\nL(y;\\theta) = f(y_1,y_2,\\ldots,y_n | \\theta) = \\prod_{i=1}^nf(y_i\\theta).\nDado que los GLM pertenecen a la familia exponencial, la verosimilitud se puede expresar de forma analítica como:\nL(y;\\theta) = \\prod_{i=1}^n \\exp \\left[y_i b(\\theta_i)+c(\\theta_i)+d(y_i) \\right].\nFinalmente, la log-verosimilitud que es el logaritmo de L, también posee forma analítica\nl(y;\\theta) = \\sum_{i=1}^n y_i b(\\theta_i) +\\sum_{i=1}^nc(\\theta_i) + \\sum_{i=1}^n d(y_i)."
  },
  {
    "objectID": "posts/GLMs/index.html#estimación-de-los-parámetros",
    "href": "posts/GLMs/index.html#estimación-de-los-parámetros",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Estimación de los parámetros",
    "text": "Estimación de los parámetros\nEl método de optimización más popular en GLMs es Máxima Verosimilitud que consiste en optimizar la log-verosimilitud, o simplemente resolver la función de score.\nU(y;\\theta) = \\frac{\\partial}{\\partial \\theta} l(y;\\theta) = 0. Generalmente resolver U implica resolver un sistema de ecuaciones no lineales, y al inicio el método más utilizado es el algoritmo de Newton. En la actualidad dicho algoritmo a evolucionado a un Limited Memory Broyden–Fletcher–Goldfarb–Shanno algorithm. El algoritmo L-BFGS es un método de Quasi-Newton que\n\nAproxima la matriz Jacobiana usando el método de Broyden, y\nAplica el algoritmo de Fletcher para corregir por estabilidad la aproximación de Broyden."
  },
  {
    "objectID": "posts/GLMs/index.html#incertidumbre-de-los-estimadores",
    "href": "posts/GLMs/index.html#incertidumbre-de-los-estimadores",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Incertidumbre de los estimadores",
    "text": "Incertidumbre de los estimadores\nUn estimador es cualquier estadístico w(Y) que se utiliza para inferir información de \\beta. Dado que los estimadores \\hat \\beta = w(Y) son transformaciones de la muestra, poseen una distribución1.\nw(Y) \\sim f(w|\\beta).\nEn la mayoría de los casos, la distribución muestral no tiene forma analítica, y esta se aproxima usando remuestreo.\nLos algoritmos de remuestreo más utilizados son\n\nJackniffe\nBootstrap\n\nDe los dos algoritmos el más popular es Bootstrap, que consiste en generar una muestra de estimadores que aproxima la distribución deseada. El algoritmo es:\n\nElegir el número de sub-muestras B\nPara b =1,2,3,\\ldots, B hacer:\n\nExtraer una sub-muestra Y_b con reemplazo de Y,\nEstimar los parámetros del modelo \\hat \\beta_b con la sub-muestra Y_b.\n\nLa colección \\hat \\beta_1,\\hat \\beta_2, \\hat \\beta_3 ,\\ldots,\\hat \\beta_B es una muestra de la distribución muestral de los estimadores \\hat \\beta.\nUsar \\hat \\beta_1,\\hat \\beta_2, \\hat \\beta_3 ,\\ldots,\\hat \\beta_B para aproximar los intervalos de confianza de \\hat \\beta.\n\nEl método de Jackniffe es un caso particular del Bootstrap que consiste en extraer la sub-muestra Y_b eliminando la b-ésima observación y_b de la muestra original.\nPara análisis de incertidumbre de los estimadores, el Bootstrap provee mejores resultados que el método de Jackniffe, pero el segundo tiene su nicho al comparar diferentes modelos, validación cruzada."
  },
  {
    "objectID": "posts/GLMs/index.html#análisis-de-los-residuos.",
    "href": "posts/GLMs/index.html#análisis-de-los-residuos.",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Análisis de los residuos.",
    "text": "Análisis de los residuos.\nLos residuos de un modelo se definen como la diferencia entre el valor ajustado por el modelo \\hat Y y su valor real Y.\nR_i = \\hat Y_i - Y_i.\nEn GLMs solo se revisan generalidades simples de los supuestos como:\n\nQue estén centrados en cero R_i \\approx 0.\nSean de varianza homogénea \\sigma_R\n\nEn modelos lineales simples, se pueden revisar más supuestos como normalidad, homogeneidad, y estacionaridad, ya que los residuos estiman los errores del modelo R_i = \\hat \\varepsilon_i.\nUn estadístico importante obtenido de los residuos, es el coeficiente de determinación R^2 que establece el porcentaje de varianza explicada por el modelo.\nR^2 = 1 - \\frac{\\sigma_R/(n-d-1)}{V[y]/(n-1)}"
  },
  {
    "objectID": "posts/GLMs/index.html#selección-de-modelos",
    "href": "posts/GLMs/index.html#selección-de-modelos",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Selección de modelos",
    "text": "Selección de modelos\nEn la práctica, es posible que se desarrollen múltiples modelos que expliquen el mismo conjunto de datos Z, existen indicadores que permiten describir las cualidades del modelo, los más utilizados son:\n\nlog-verosimilitud: \\log L= -l(y;\\theta).\nCriterio de Información de Bayes: BIC = 2 \\log L - 2\\log(n^d)\nCriterio de Información de Akaike AIC - 2 \\log L -2 d\nRoot Mean square error RMSE = \\frac{1}{\\sqrt n}||R||_2\nMean Absolute Error MAE = \\frac{1}{n}||R||_1\nMean Absolute Percentaje Error MAPE =\\frac{1}{n}\\sum_{i=1}^n \\left| \\frac{R_i}{Y_i} \\right|\n\n\nSelección de variables\nUna aplicación de la selección de modelos es encontrar modelos reducidos, esto es encontrar un subconjunto de variables X_1 \\subset X tal que el modelo reducido M_r sea lo mas parsimonioso posible. Un modelo parsimonioso brinda mayor explicabilidad, mayor capacidad de aprendizaje y mayor capacidad de generalización.\nLos métodos para reducción de variables se les conoce como búsquedas, y los tipos son:\n\nBúsquedas hacia adelante\nBúsquedas hacia atrás.\n\nEn búsqueda hacia adelante se inicia con el modelo más pequeño posible, se mide su criterio de información, y luego se agregan variables de forma secuencial de tal forma que el criterio mejore."
  },
  {
    "objectID": "posts/GLMs/index.html#aprendizaje",
    "href": "posts/GLMs/index.html#aprendizaje",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Aprendizaje",
    "text": "Aprendizaje\nLos indicadores presentados miden el ajuste del modelo, esto es la capacidad del modelo de explicar el conjunto de datos. En aplicaciones mas recientes, la explicabilidad es una propiedad poco deseable, se prefiere medir la capacidad de aprendizaje.\n\n\n\n\n\n\nAprendizaje\n\n\n\nLa capacidad de aprendizaje es la habilidad del modelo de explicar propiedades externas a partir de la información adquirida.\n\n\nEn términos probabilistas, es la habilidad del modelo de predecir un nuevo conjunto de datos, a partir de los datos disponibles.\n\nParticiones\nEl aprendizaje se puede medir usando una partición de los datos. Sea Y = \\{Y_1,Y_2,\\ldots,Y_n\\} una muestra aleatoria, definimos una partición de entrenamiento y prueba como\nY = Y_E \\bigcup Y_P Donde:\n\nY_E \\bigcap Y_P = \\emptyset.\nm+k = n y m >> k.\nY_E = \\{Y_1,Y_2,\\ldots,Y_m\\} \\subset Y es el conjunto de entrenamiento.\nY_P = \\{Y_1,Y_2,\\ldots,Y_k\\} \\subset Y es el conjunto de prueba.\n\nEl algoritmo para medir aprendizaje es:\n\nAjustar los modelos M_1,M_2,\\ldots,M_f usando el conjunto de entrenamiento Y_E.\nPara cada modelo M_j hacer:\n\nRealizar k predicciones \\hat Y_{P,1},\\hat Y_{P,2},\\ldots,\\hat Y_{P,k}.\nComparar \\hat Y_{P} con Y_P usando AIC, BIC log-lik, RMSE o MAPE.\n\nEl modelo con mayor aprendizaje es el modelo con criterio menor.\n\nLa mayor limitante de las particiones es que al ser aleatorias los resultados varian bastante según la selección de las observaciones en el conjunto de entrenamiento. Una forma de evitar esos problemas es usando Validación cruzada."
  },
  {
    "objectID": "posts/GLMs/index.html#validación-cruzada",
    "href": "posts/GLMs/index.html#validación-cruzada",
    "title": "Modelos Lineales Generalizados Aplicados",
    "section": "Validación cruzada",
    "text": "Validación cruzada\nValidación Cruzada consiste en realizar el proceso de partición muchas veces, el método mas utilizados es k-fold cross-validation este consisten en:\n\nDefinir m como el número de iteraciones a realizar\nPara i = 1,2,3,\\ldots, m hacer:\n\ndefinir el conjunto de prueba Y_p de tamaño k.\ndefinir el conjunto de entrenamiento Y_E como el complemento Y_E = Y_P^c.\nEstimar los modelos con Y_E\nComparar las predicciones de cada modelo con Y_P mediante algún criterio de información.\n\nPromediar los criterios obtenidos de cada iteración.\nElegir el modelo con criterio promedio menor.\n\nCuando el conjunto e prueba solo posee una observación, al método se le conoce como LOO (Leave one out cross validation). Otro método es utilizar un Bootstrap pero es altamente costoso.\n\nNotar que:\n\nLOO es equivalente a un muestreo de Jackniffe.\nCon validación cruzada re-utilizamos la información\nUsamos toda la muestra para validar los resultados.\nSe minimiza la variación de errores de aprendizaje."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Bayesian Modeling",
    "section": "",
    "text": "Gaussian\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRegression\n\n\ncode\n\n\nGLMs\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nAsael Alonzo Matamoros.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\nintroduction\n\n\nBaseline\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nAsael Alonzo Matamoros\n\n\n\n\n\n\nNo matching items"
  }
]